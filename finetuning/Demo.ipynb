{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b7de143",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/sdsc-bw/DataFactory/blob/develop/finetuning/Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fe1f22",
   "metadata": {},
   "source": [
    "# Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7d63d5",
   "metadata": {},
   "source": [
    "To find a suitable model for your ML problem is very important. Not every model has the same performane on every task. Some models can be to simple (underfitting) and some models can be to complex for for a problem (overfitting). Also a model has different hyperparameters which also have an impact on the performance. Therefor exist libraries that can be used to find a appropriate model and its hyperparameters. Popular ones are [auto-sklearn](https://papers.neurips.cc/paper/2015/hash/11d0e6287202fced83f79975ec59a3a6-Abstract.html) and [hyperopt](https://github.com/hyperopt/hyperopt)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8364c25",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e2df4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8205e07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in sys.modules:\n",
    "    !git clone https://github.com/sdsc-bw/DataFactory.git\n",
    "    !ls\n",
    "    \n",
    "    !sudo apt-get install build-essential swig\n",
    "    !curl https://raw.githubusercontent.com/automl/auto-sklearn/master/requirements.txt | xargs -n 1 -L 1 pip install\n",
    "    !pip install auto-sklearn\n",
    "    \n",
    "    !pip install scipy\n",
    "    \n",
    "    !pip install tsai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46a0a167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9289706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os             : Windows-10-10.0.19041-SP0\n",
      "python         : 3.8.12\n",
      "tsai           : 0.2.23\n",
      "fastai         : 2.5.3\n",
      "fastcore       : 1.3.27\n",
      "torch          : 1.9.1+cpu\n",
      "n_cpus         : 8\n",
      "device         : cpu\n",
      "os             : Windows-10-10.0.19041-SP0\n",
      "python         : 3.8.12\n",
      "tsai           : 0.2.23\n",
      "fastai         : 2.5.3\n",
      "fastcore       : 1.3.27\n",
      "torch          : 1.9.1+cpu\n",
      "n_cpus         : 8\n",
      "device         : cpu\n",
      "os             : Windows-10-10.0.19041-SP0\n",
      "python         : 3.8.12\n",
      "tsai           : 0.2.23\n",
      "fastai         : 2.5.3\n",
      "fastcore       : 1.3.27\n",
      "torch          : 1.9.1+cpu\n",
      "n_cpus         : 8\n",
      "device         : cpu\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.utils import shuffle\n",
    "from hyperopt import hp\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    root = 'DataFactory/'\n",
    "else:\n",
    "    root = '../'\n",
    "    \n",
    "sys.path.insert(0, root + \"codes\")\n",
    "\n",
    "from DataFactory import DataFactory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9259c4c0",
   "metadata": {},
   "source": [
    "## Load dataset: Diabetes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec311175",
   "metadata": {},
   "outputs": [],
   "source": [
    "datafactory = DataFactory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0eb83d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-21 15:51:21,112 - DataFactory - INFO - Remove columns with NAN-values of target feature: class\n",
      "2021-12-21 15:51:21,115 - DataFactory - INFO - Start to transform the categorical columns...\n",
      "2021-12-21 15:51:21,118 - DataFactory - INFO - ...End with categorical feature transformation\n",
      "2021-12-21 15:51:21,119 - DataFactory - INFO - Start to clean the given dataframe...\n",
      "2021-12-21 15:51:21,120 - DataFactory - INFO - Number of INF- and NAN-values are: (0, 0)\n",
      "2021-12-21 15:51:21,121 - DataFactory - INFO - Set type to float32 at first && deal with INF\n",
      "2021-12-21 15:51:21,122 - DataFactory - INFO - Remove columns with half of NAN-values\n",
      "2021-12-21 15:51:21,125 - DataFactory - INFO - Remove constant columns\n",
      "2021-12-21 15:51:21,129 - DataFactory - INFO - ...End with Data cleaning, number of INF- and NAN-values are now: (0, 0)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_wine()\n",
    "df = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
    "df['class'] = pd.Series(dataset.target)\n",
    "df = shuffle(df)\n",
    "X, y = datafactory.preprocess(df, y_col='class')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527d1d73",
   "metadata": {},
   "source": [
    "## How To Use DataFactory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5253ff",
   "metadata": {},
   "source": [
    "### Hyperopt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100cca6b",
   "metadata": {},
   "source": [
    "We provided a function to use hyperopt. Some of the models require finetuning with hyperopt. \n",
    "\n",
    "We can define the models that we want to test. Then we have to define a *params* variable that defines the strategy how to examine the search space. There we also can define the parameters of the search space. If parameters for models are not given, it uses our standard search space. Like we do it here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "515994ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list with models to try out\n",
    "models = ['decision_tree', 'random_forest', 'adaboost', 'inception_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da3da630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 32/32 [06:54<00:00, 12.96s/trial, best loss: -0.9774952919020716]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Score</th>\n",
       "      <th>Hyperparams</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.977495</td>\n",
       "      <td>{'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50}</td>\n",
       "      <td>0.185534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.966196</td>\n",
       "      <td>{'max_depth': 5, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 200}</td>\n",
       "      <td>0.623314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.960640</td>\n",
       "      <td>{'max_depth': 20, 'min_samples_leaf': 4, 'min_samples_split': 5, 'n_estimators': 50}</td>\n",
       "      <td>0.268947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.960640</td>\n",
       "      <td>{'max_depth': 2, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}</td>\n",
       "      <td>0.414587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.960546</td>\n",
       "      <td>{'max_depth': 1, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}</td>\n",
       "      <td>0.298953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.960546</td>\n",
       "      <td>{'max_depth': 1, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 200}</td>\n",
       "      <td>0.594014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.960546</td>\n",
       "      <td>{'max_depth': 50, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 100}</td>\n",
       "      <td>0.322212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.954991</td>\n",
       "      <td>{'max_depth': 1, 'min_samples_leaf': 4, 'min_samples_split': 5, 'n_estimators': 50}</td>\n",
       "      <td>0.152612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.949341</td>\n",
       "      <td>{'max_depth': 1, 'min_samples_leaf': 4, 'min_samples_split': 5, 'n_estimators': 200}</td>\n",
       "      <td>0.590016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.949247</td>\n",
       "      <td>{'max_depth': 1, 'min_samples_leaf': 4, 'min_samples_split': 5, 'n_estimators': 50}</td>\n",
       "      <td>0.154468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>adaboost</td>\n",
       "      <td>0.927024</td>\n",
       "      <td>{'learning_rate': 0.01, 'n_estimators': 200}</td>\n",
       "      <td>0.672221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>adaboost</td>\n",
       "      <td>0.927024</td>\n",
       "      <td>{'learning_rate': 0.01, 'n_estimators': 200}</td>\n",
       "      <td>0.772962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>decision_tree</td>\n",
       "      <td>0.926836</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 6.0, 'min_samples_leaf': 1, 'min_samples_split': 3}</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>decision_tree</td>\n",
       "      <td>0.926836</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 10.0, 'min_samples_leaf': 1, 'min_samples_split': 2}</td>\n",
       "      <td>0.020945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>decision_tree</td>\n",
       "      <td>0.915443</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 7.0, 'min_samples_leaf': 2, 'min_samples_split': 2}</td>\n",
       "      <td>0.010971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>adaboost</td>\n",
       "      <td>0.910264</td>\n",
       "      <td>{'learning_rate': 0.1, 'n_estimators': 50}</td>\n",
       "      <td>0.175534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>adaboost</td>\n",
       "      <td>0.910264</td>\n",
       "      <td>{'learning_rate': 0.1, 'n_estimators': 50}</td>\n",
       "      <td>0.243350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>adaboost</td>\n",
       "      <td>0.876271</td>\n",
       "      <td>{'learning_rate': 0.01, 'n_estimators': 50}</td>\n",
       "      <td>0.175532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>adaboost</td>\n",
       "      <td>0.876271</td>\n",
       "      <td>{'learning_rate': 0.01, 'n_estimators': 50}</td>\n",
       "      <td>0.173536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>decision_tree</td>\n",
       "      <td>0.870245</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 5.0, 'min_samples_leaf': 4, 'min_samples_split': 2}</td>\n",
       "      <td>0.021941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>decision_tree</td>\n",
       "      <td>0.854331</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 3.0, 'min_samples_leaf': 2, 'min_samples_split': 2}</td>\n",
       "      <td>0.019947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>inception_time_plus</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>{'arch_config': {'nb_filters': 64, 'nf': 32}, 'loss_func': 'cross_entropy', 'opt_func': 'adam', 'epochs': 25, 'lr_max': 0.001}</td>\n",
       "      <td>27.009762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>adaboost</td>\n",
       "      <td>0.849247</td>\n",
       "      <td>{'learning_rate': 0.1, 'n_estimators': 100}</td>\n",
       "      <td>0.336835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>decision_tree</td>\n",
       "      <td>0.837288</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 3.0, 'min_samples_leaf': 4, 'min_samples_split': 5}</td>\n",
       "      <td>0.009976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>inception_time_plus</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>{'arch_config': {'nb_filters': 32, 'nf': 32}, 'loss_func': 'cross_entropy', 'opt_func': 'adam', 'epochs': 25, 'lr_max': 0.001}</td>\n",
       "      <td>17.609313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>adaboost</td>\n",
       "      <td>0.808569</td>\n",
       "      <td>{'learning_rate': 0.001, 'n_estimators': 200}</td>\n",
       "      <td>0.639290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>inception_time_plus</td>\n",
       "      <td>0.759259</td>\n",
       "      <td>{'arch_config': {'nb_filters': 96, 'nf': 32}, 'loss_func': 'cross_entropy', 'opt_func': 'adam', 'epochs': 25, 'lr_max': 0.001}</td>\n",
       "      <td>58.430174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>inception_time_plus</td>\n",
       "      <td>0.731481</td>\n",
       "      <td>{'arch_config': {'nb_filters': 96, 'nf': 32}, 'loss_func': 'cross_entropy', 'opt_func': 'adam', 'epochs': 25, 'lr_max': 0.001}</td>\n",
       "      <td>54.067436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>inception_time_plus</td>\n",
       "      <td>0.703704</td>\n",
       "      <td>{'arch_config': {'nb_filters': 128, 'nf': 64}, 'loss_func': 'cross_entropy', 'opt_func': 'adam', 'epochs': 25, 'lr_max': 0.001}</td>\n",
       "      <td>93.442278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>inception_time_plus</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>{'arch_config': {'nb_filters': 128, 'nf': 32}, 'loss_func': 'cross_entropy', 'opt_func': 'adam', 'epochs': 25, 'lr_max': 0.001}</td>\n",
       "      <td>97.216167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>inception_time_plus</td>\n",
       "      <td>0.648148</td>\n",
       "      <td>{'arch_config': {'nb_filters': 96, 'nf': 64}, 'loss_func': 'cross_entropy', 'opt_func': 'adam', 'epochs': 25, 'lr_max': 0.001}</td>\n",
       "      <td>59.906034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>decision_tree</td>\n",
       "      <td>0.595574</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 1.0, 'min_samples_leaf': 4, 'min_samples_split': 3}</td>\n",
       "      <td>0.010971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model     Score  \\\n",
       "0         random_forest  0.977495   \n",
       "1         random_forest  0.966196   \n",
       "2         random_forest  0.960640   \n",
       "3         random_forest  0.960640   \n",
       "4         random_forest  0.960546   \n",
       "5         random_forest  0.960546   \n",
       "6         random_forest  0.960546   \n",
       "7         random_forest  0.954991   \n",
       "8         random_forest  0.949341   \n",
       "9         random_forest  0.949247   \n",
       "10             adaboost  0.927024   \n",
       "11             adaboost  0.927024   \n",
       "12        decision_tree  0.926836   \n",
       "13        decision_tree  0.926836   \n",
       "14        decision_tree  0.915443   \n",
       "15             adaboost  0.910264   \n",
       "16             adaboost  0.910264   \n",
       "17             adaboost  0.876271   \n",
       "18             adaboost  0.876271   \n",
       "19        decision_tree  0.870245   \n",
       "20        decision_tree  0.854331   \n",
       "21  inception_time_plus  0.851852   \n",
       "22             adaboost  0.849247   \n",
       "23        decision_tree  0.837288   \n",
       "24  inception_time_plus  0.833333   \n",
       "25             adaboost  0.808569   \n",
       "26  inception_time_plus  0.759259   \n",
       "27  inception_time_plus  0.731481   \n",
       "28  inception_time_plus  0.703704   \n",
       "29  inception_time_plus  0.666667   \n",
       "30  inception_time_plus  0.648148   \n",
       "31        decision_tree  0.595574   \n",
       "\n",
       "                                                                                                                        Hyperparams  \\\n",
       "0                                               {'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50}   \n",
       "1                                             {'max_depth': 5, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 200}   \n",
       "2                                              {'max_depth': 20, 'min_samples_leaf': 4, 'min_samples_split': 5, 'n_estimators': 50}   \n",
       "3                                              {'max_depth': 2, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}   \n",
       "4                                              {'max_depth': 1, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}   \n",
       "5                                             {'max_depth': 1, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 200}   \n",
       "6                                             {'max_depth': 50, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 100}   \n",
       "7                                               {'max_depth': 1, 'min_samples_leaf': 4, 'min_samples_split': 5, 'n_estimators': 50}   \n",
       "8                                              {'max_depth': 1, 'min_samples_leaf': 4, 'min_samples_split': 5, 'n_estimators': 200}   \n",
       "9                                               {'max_depth': 1, 'min_samples_leaf': 4, 'min_samples_split': 5, 'n_estimators': 50}   \n",
       "10                                                                                     {'learning_rate': 0.01, 'n_estimators': 200}   \n",
       "11                                                                                     {'learning_rate': 0.01, 'n_estimators': 200}   \n",
       "12                                        {'criterion': 'entropy', 'max_depth': 6.0, 'min_samples_leaf': 1, 'min_samples_split': 3}   \n",
       "13                                       {'criterion': 'entropy', 'max_depth': 10.0, 'min_samples_leaf': 1, 'min_samples_split': 2}   \n",
       "14                                        {'criterion': 'entropy', 'max_depth': 7.0, 'min_samples_leaf': 2, 'min_samples_split': 2}   \n",
       "15                                                                                       {'learning_rate': 0.1, 'n_estimators': 50}   \n",
       "16                                                                                       {'learning_rate': 0.1, 'n_estimators': 50}   \n",
       "17                                                                                      {'learning_rate': 0.01, 'n_estimators': 50}   \n",
       "18                                                                                      {'learning_rate': 0.01, 'n_estimators': 50}   \n",
       "19                                        {'criterion': 'entropy', 'max_depth': 5.0, 'min_samples_leaf': 4, 'min_samples_split': 2}   \n",
       "20                                        {'criterion': 'entropy', 'max_depth': 3.0, 'min_samples_leaf': 2, 'min_samples_split': 2}   \n",
       "21   {'arch_config': {'nb_filters': 64, 'nf': 32}, 'loss_func': 'cross_entropy', 'opt_func': 'adam', 'epochs': 25, 'lr_max': 0.001}   \n",
       "22                                                                                      {'learning_rate': 0.1, 'n_estimators': 100}   \n",
       "23                                        {'criterion': 'entropy', 'max_depth': 3.0, 'min_samples_leaf': 4, 'min_samples_split': 5}   \n",
       "24   {'arch_config': {'nb_filters': 32, 'nf': 32}, 'loss_func': 'cross_entropy', 'opt_func': 'adam', 'epochs': 25, 'lr_max': 0.001}   \n",
       "25                                                                                    {'learning_rate': 0.001, 'n_estimators': 200}   \n",
       "26   {'arch_config': {'nb_filters': 96, 'nf': 32}, 'loss_func': 'cross_entropy', 'opt_func': 'adam', 'epochs': 25, 'lr_max': 0.001}   \n",
       "27   {'arch_config': {'nb_filters': 96, 'nf': 32}, 'loss_func': 'cross_entropy', 'opt_func': 'adam', 'epochs': 25, 'lr_max': 0.001}   \n",
       "28  {'arch_config': {'nb_filters': 128, 'nf': 64}, 'loss_func': 'cross_entropy', 'opt_func': 'adam', 'epochs': 25, 'lr_max': 0.001}   \n",
       "29  {'arch_config': {'nb_filters': 128, 'nf': 32}, 'loss_func': 'cross_entropy', 'opt_func': 'adam', 'epochs': 25, 'lr_max': 0.001}   \n",
       "30   {'arch_config': {'nb_filters': 96, 'nf': 64}, 'loss_func': 'cross_entropy', 'opt_func': 'adam', 'epochs': 25, 'lr_max': 0.001}   \n",
       "31                                        {'criterion': 'entropy', 'max_depth': 1.0, 'min_samples_leaf': 4, 'min_samples_split': 3}   \n",
       "\n",
       "         Time  \n",
       "0    0.185534  \n",
       "1    0.623314  \n",
       "2    0.268947  \n",
       "3    0.414587  \n",
       "4    0.298953  \n",
       "5    0.594014  \n",
       "6    0.322212  \n",
       "7    0.152612  \n",
       "8    0.590016  \n",
       "9    0.154468  \n",
       "10   0.672221  \n",
       "11   0.772962  \n",
       "12   0.010000  \n",
       "13   0.020945  \n",
       "14   0.010971  \n",
       "15   0.175534  \n",
       "16   0.243350  \n",
       "17   0.175532  \n",
       "18   0.173536  \n",
       "19   0.021941  \n",
       "20   0.019947  \n",
       "21  27.009762  \n",
       "22   0.336835  \n",
       "23   0.009976  \n",
       "24  17.609313  \n",
       "25   0.639290  \n",
       "26  58.430174  \n",
       "27  54.067436  \n",
       "28  93.442278  \n",
       "29  97.216167  \n",
       "30  59.906034  \n",
       "31   0.010971  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loss in this case refers to -f1\n",
    "model = datafactory.finetune(X, y, method='hyperopt', models=models, cv=3, max_evals=32, mtype='C')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2265eb63",
   "metadata": {},
   "source": [
    "If we want to define custom parameters, they should be defined with the functions of hyperopt. Look at the [sklearn](https://scikit-learn.org/stable/) and [tsai](https://github.com/timeseriesAI/tsai) website to find the hyperparamters of the models. Attention: The identifier of the hyperparameters need to be unique for ever parameter (also between models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b60853f",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['decision_tree', 'random_forest', 'adaboost', 'inception_time']\n",
    "# attention the label has to be unique for every parameter (also between models)\n",
    "dt_params = {'max_depth': hp.quniform('max_depth_dt', 1, 10, 1), 'criterion': hp.choice('criterion_dt', ['gini', 'entropy']), 'min_samples_leaf': hp.choice('min_samples_leaf_dt', [1, 2, 4])}\n",
    "rf_params = {'max_depth': hp.choice('max_depth_rf', [1, 2, 3, 5, 10, 20, 50]), 'n_estimators': hp.choice('n_estimators_rf', [50, 100, 200])}\n",
    "ab_params = {'n_estimators': hp.choice('n_estimators_ab', [50, 100, 200]), 'learning_rate': hp.choice('learning_rate_ab', [0.001,0.01,.1,1.0])}\n",
    "# tsai uses a learner, you also can finetune its parameters\n",
    "it_learner_params = {'epochs': hp.choice('epochs_it', [25, 50]), 'lr_max': hp.choice('lr_max_it', [1e-3, 1e-5]), 'opt_func':  hp.choice('optimizer_it', ['adam']), 'loss_func': hp.choice('loss_it', ['mse']), 'batch_tfms': ['standardize'], 'batch_size': [64, 128], 'splits': None, 'metrics': ['accuracy']}\n",
    "it_params = {'learner': it_learner_params, 'nf': hp.choice('nf_it', [32, 64]), 'nb_filters': hp.choice('nb_filters_it', [32, 64, 96, 128])}\n",
    "# put every hyperparameter definition in an own dictionary\n",
    "# search strategy of hyperparameters should be in ['parzen', 'random']\n",
    "params = {'strategy': 'random', 'decision_tree': dt_params, 'random_forest': rf_params, 'adaboost': ab_params, 'inception_time': it_params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3676483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 32/32 [10:45<00:00, 20.19s/trial, best loss: -0.9775894538606403]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Score</th>\n",
       "      <th>Hyperparams</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.977589</td>\n",
       "      <td>{'max_depth': 10, 'n_estimators': 50}</td>\n",
       "      <td>0.161161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.971940</td>\n",
       "      <td>{'max_depth': 2, 'n_estimators': 100}</td>\n",
       "      <td>0.317152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.971940</td>\n",
       "      <td>{'max_depth': 20, 'n_estimators': 100}</td>\n",
       "      <td>0.434929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.971846</td>\n",
       "      <td>{'max_depth': 5, 'n_estimators': 100}</td>\n",
       "      <td>0.307173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.938041</td>\n",
       "      <td>{'max_depth': 1, 'n_estimators': 100}</td>\n",
       "      <td>0.391893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>decision_tree</td>\n",
       "      <td>0.926836</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 5.0, 'min_samples_leaf': 1}</td>\n",
       "      <td>0.010923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.926648</td>\n",
       "      <td>{'max_depth': 1, 'n_estimators': 50}</td>\n",
       "      <td>0.165067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>decision_tree</td>\n",
       "      <td>0.921375</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 4.0, 'min_samples_leaf': 1}</td>\n",
       "      <td>0.020945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>adaboost</td>\n",
       "      <td>0.910264</td>\n",
       "      <td>{'learning_rate': 0.1, 'n_estimators': 50}</td>\n",
       "      <td>0.174531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>adaboost</td>\n",
       "      <td>0.910264</td>\n",
       "      <td>{'learning_rate': 0.1, 'n_estimators': 50}</td>\n",
       "      <td>0.257342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>decision_tree</td>\n",
       "      <td>0.898399</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 6.0, 'min_samples_leaf': 4}</td>\n",
       "      <td>0.020945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>decision_tree</td>\n",
       "      <td>0.898399</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 9.0, 'min_samples_leaf': 4}</td>\n",
       "      <td>0.020943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>decision_tree</td>\n",
       "      <td>0.887100</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 5.0, 'min_samples_leaf': 4}</td>\n",
       "      <td>0.022940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>decision_tree</td>\n",
       "      <td>0.887100</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 6.0, 'min_samples_leaf': 4}</td>\n",
       "      <td>0.009973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>decision_tree</td>\n",
       "      <td>0.876177</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 4.0, 'min_samples_leaf': 2}</td>\n",
       "      <td>0.019946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>decision_tree</td>\n",
       "      <td>0.875895</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 6.0, 'min_samples_leaf': 4}</td>\n",
       "      <td>0.020944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>decision_tree</td>\n",
       "      <td>0.859981</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 3.0, 'min_samples_leaf': 1}</td>\n",
       "      <td>0.022450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>adaboost</td>\n",
       "      <td>0.849247</td>\n",
       "      <td>{'learning_rate': 0.1, 'n_estimators': 200}</td>\n",
       "      <td>0.772584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>adaboost</td>\n",
       "      <td>0.849247</td>\n",
       "      <td>{'learning_rate': 0.1, 'n_estimators': 100}</td>\n",
       "      <td>0.429737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>inception_time_plus</td>\n",
       "      <td>0.824074</td>\n",
       "      <td>{'arch_config': {'nb_filters': 32, 'nf': 64}, 'loss_func': 'cross_entropy', 'opt_func': 'adam', 'epochs': 25, 'lr_max': 0.001}</td>\n",
       "      <td>15.550421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>inception_time_plus</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>{'arch_config': {'nb_filters': 32, 'nf': 64}, 'loss_func': 'cross_entropy', 'opt_func': 'adam', 'epochs': 25, 'lr_max': 0.001}</td>\n",
       "      <td>16.808399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>adaboost</td>\n",
       "      <td>0.808569</td>\n",
       "      <td>{'learning_rate': 0.001, 'n_estimators': 200}</td>\n",
       "      <td>0.676664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>adaboost</td>\n",
       "      <td>0.808569</td>\n",
       "      <td>{'learning_rate': 0.001, 'n_estimators': 200}</td>\n",
       "      <td>0.687316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>inception_time_plus</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>{'arch_config': {'nb_filters': 64, 'nf': 64}, 'loss_func': 'cross_entropy', 'opt_func': 'adam', 'epochs': 25, 'lr_max': 0.001}</td>\n",
       "      <td>29.486584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>inception_time_plus</td>\n",
       "      <td>0.768519</td>\n",
       "      <td>{'arch_config': {'nb_filters': 96, 'nf': 64}, 'loss_func': 'cross_entropy', 'opt_func': 'adam', 'epochs': 25, 'lr_max': 0.001}</td>\n",
       "      <td>59.597891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>inception_time_plus</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>{'arch_config': {'nb_filters': 128, 'nf': 32}, 'loss_func': 'cross_entropy', 'opt_func': 'adam', 'epochs': 25, 'lr_max': 0.001}</td>\n",
       "      <td>95.304753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>inception_time_plus</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>{'arch_config': {'nb_filters': 128, 'nf': 64}, 'loss_func': 'cross_entropy', 'opt_func': 'adam', 'epochs': 25, 'lr_max': 0.001}</td>\n",
       "      <td>93.016150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>inception_time_plus</td>\n",
       "      <td>0.703704</td>\n",
       "      <td>{'arch_config': {'nb_filters': 128, 'nf': 32}, 'loss_func': 'cross_entropy', 'opt_func': 'adam', 'epochs': 25, 'lr_max': 0.001}</td>\n",
       "      <td>90.920858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>inception_time_plus</td>\n",
       "      <td>0.685185</td>\n",
       "      <td>{'arch_config': {'nb_filters': 96, 'nf': 32}, 'loss_func': 'cross_entropy', 'opt_func': 'adam', 'epochs': 25, 'lr_max': 0.001}</td>\n",
       "      <td>53.342346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>inception_time_plus</td>\n",
       "      <td>0.675926</td>\n",
       "      <td>{'arch_config': {'nb_filters': 128, 'nf': 32}, 'loss_func': 'cross_entropy', 'opt_func': 'adam', 'epochs': 25, 'lr_max': 0.001}</td>\n",
       "      <td>91.325003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>inception_time_plus</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>{'arch_config': {'nb_filters': 96, 'nf': 64}, 'loss_func': 'cross_entropy', 'opt_func': 'adam', 'epochs': 25, 'lr_max': 0.001}</td>\n",
       "      <td>58.513602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>inception_time_plus</td>\n",
       "      <td>0.657407</td>\n",
       "      <td>{'arch_config': {'nb_filters': 64, 'nf': 64}, 'loss_func': 'cross_entropy', 'opt_func': 'adam', 'epochs': 25, 'lr_max': 0.001}</td>\n",
       "      <td>37.041690</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model     Score  \\\n",
       "0         random_forest  0.977589   \n",
       "1         random_forest  0.971940   \n",
       "2         random_forest  0.971940   \n",
       "3         random_forest  0.971846   \n",
       "4         random_forest  0.938041   \n",
       "5         decision_tree  0.926836   \n",
       "6         random_forest  0.926648   \n",
       "7         decision_tree  0.921375   \n",
       "8              adaboost  0.910264   \n",
       "9              adaboost  0.910264   \n",
       "10        decision_tree  0.898399   \n",
       "11        decision_tree  0.898399   \n",
       "12        decision_tree  0.887100   \n",
       "13        decision_tree  0.887100   \n",
       "14        decision_tree  0.876177   \n",
       "15        decision_tree  0.875895   \n",
       "16        decision_tree  0.859981   \n",
       "17             adaboost  0.849247   \n",
       "18             adaboost  0.849247   \n",
       "19  inception_time_plus  0.824074   \n",
       "20  inception_time_plus  0.814815   \n",
       "21             adaboost  0.808569   \n",
       "22             adaboost  0.808569   \n",
       "23  inception_time_plus  0.777778   \n",
       "24  inception_time_plus  0.768519   \n",
       "25  inception_time_plus  0.750000   \n",
       "26  inception_time_plus  0.740741   \n",
       "27  inception_time_plus  0.703704   \n",
       "28  inception_time_plus  0.685185   \n",
       "29  inception_time_plus  0.675926   \n",
       "30  inception_time_plus  0.666667   \n",
       "31  inception_time_plus  0.657407   \n",
       "\n",
       "                                                                                                                        Hyperparams  \\\n",
       "0                                                                                             {'max_depth': 10, 'n_estimators': 50}   \n",
       "1                                                                                             {'max_depth': 2, 'n_estimators': 100}   \n",
       "2                                                                                            {'max_depth': 20, 'n_estimators': 100}   \n",
       "3                                                                                             {'max_depth': 5, 'n_estimators': 100}   \n",
       "4                                                                                             {'max_depth': 1, 'n_estimators': 100}   \n",
       "5                                                                 {'criterion': 'entropy', 'max_depth': 5.0, 'min_samples_leaf': 1}   \n",
       "6                                                                                              {'max_depth': 1, 'n_estimators': 50}   \n",
       "7                                                                 {'criterion': 'entropy', 'max_depth': 4.0, 'min_samples_leaf': 1}   \n",
       "8                                                                                        {'learning_rate': 0.1, 'n_estimators': 50}   \n",
       "9                                                                                        {'learning_rate': 0.1, 'n_estimators': 50}   \n",
       "10                                                                   {'criterion': 'gini', 'max_depth': 6.0, 'min_samples_leaf': 4}   \n",
       "11                                                                   {'criterion': 'gini', 'max_depth': 9.0, 'min_samples_leaf': 4}   \n",
       "12                                                                {'criterion': 'entropy', 'max_depth': 5.0, 'min_samples_leaf': 4}   \n",
       "13                                                                {'criterion': 'entropy', 'max_depth': 6.0, 'min_samples_leaf': 4}   \n",
       "14                                                                   {'criterion': 'gini', 'max_depth': 4.0, 'min_samples_leaf': 2}   \n",
       "15                                                                {'criterion': 'entropy', 'max_depth': 6.0, 'min_samples_leaf': 4}   \n",
       "16                                                                {'criterion': 'entropy', 'max_depth': 3.0, 'min_samples_leaf': 1}   \n",
       "17                                                                                      {'learning_rate': 0.1, 'n_estimators': 200}   \n",
       "18                                                                                      {'learning_rate': 0.1, 'n_estimators': 100}   \n",
       "19   {'arch_config': {'nb_filters': 32, 'nf': 64}, 'loss_func': 'cross_entropy', 'opt_func': 'adam', 'epochs': 25, 'lr_max': 0.001}   \n",
       "20   {'arch_config': {'nb_filters': 32, 'nf': 64}, 'loss_func': 'cross_entropy', 'opt_func': 'adam', 'epochs': 25, 'lr_max': 0.001}   \n",
       "21                                                                                    {'learning_rate': 0.001, 'n_estimators': 200}   \n",
       "22                                                                                    {'learning_rate': 0.001, 'n_estimators': 200}   \n",
       "23   {'arch_config': {'nb_filters': 64, 'nf': 64}, 'loss_func': 'cross_entropy', 'opt_func': 'adam', 'epochs': 25, 'lr_max': 0.001}   \n",
       "24   {'arch_config': {'nb_filters': 96, 'nf': 64}, 'loss_func': 'cross_entropy', 'opt_func': 'adam', 'epochs': 25, 'lr_max': 0.001}   \n",
       "25  {'arch_config': {'nb_filters': 128, 'nf': 32}, 'loss_func': 'cross_entropy', 'opt_func': 'adam', 'epochs': 25, 'lr_max': 0.001}   \n",
       "26  {'arch_config': {'nb_filters': 128, 'nf': 64}, 'loss_func': 'cross_entropy', 'opt_func': 'adam', 'epochs': 25, 'lr_max': 0.001}   \n",
       "27  {'arch_config': {'nb_filters': 128, 'nf': 32}, 'loss_func': 'cross_entropy', 'opt_func': 'adam', 'epochs': 25, 'lr_max': 0.001}   \n",
       "28   {'arch_config': {'nb_filters': 96, 'nf': 32}, 'loss_func': 'cross_entropy', 'opt_func': 'adam', 'epochs': 25, 'lr_max': 0.001}   \n",
       "29  {'arch_config': {'nb_filters': 128, 'nf': 32}, 'loss_func': 'cross_entropy', 'opt_func': 'adam', 'epochs': 25, 'lr_max': 0.001}   \n",
       "30   {'arch_config': {'nb_filters': 96, 'nf': 64}, 'loss_func': 'cross_entropy', 'opt_func': 'adam', 'epochs': 25, 'lr_max': 0.001}   \n",
       "31   {'arch_config': {'nb_filters': 64, 'nf': 64}, 'loss_func': 'cross_entropy', 'opt_func': 'adam', 'epochs': 25, 'lr_max': 0.001}   \n",
       "\n",
       "         Time  \n",
       "0    0.161161  \n",
       "1    0.317152  \n",
       "2    0.434929  \n",
       "3    0.307173  \n",
       "4    0.391893  \n",
       "5    0.010923  \n",
       "6    0.165067  \n",
       "7    0.020945  \n",
       "8    0.174531  \n",
       "9    0.257342  \n",
       "10   0.020945  \n",
       "11   0.020943  \n",
       "12   0.022940  \n",
       "13   0.009973  \n",
       "14   0.019946  \n",
       "15   0.020944  \n",
       "16   0.022450  \n",
       "17   0.772584  \n",
       "18   0.429737  \n",
       "19  15.550421  \n",
       "20  16.808399  \n",
       "21   0.676664  \n",
       "22   0.687316  \n",
       "23  29.486584  \n",
       "24  59.597891  \n",
       "25  95.304753  \n",
       "26  93.016150  \n",
       "27  90.920858  \n",
       "28  53.342346  \n",
       "29  91.325003  \n",
       "30  58.513602  \n",
       "31  37.041690  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = datafactory.finetune(X, y, method='hyperopt', models=models, cv=3, mtype='C', params=params.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9612271e",
   "metadata": {},
   "source": [
    "### Native Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a96a3b",
   "metadata": {},
   "source": [
    "Sklearn and TSAI also provide functions/propose methods to tune the hyperparameters for a specific model. We implemented a function that uses them to find the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56cfafde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list with models to try out\n",
    "models = ['decision_tree', 'random_forest', 'adaboost']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52cb31c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Best Score</th>\n",
       "      <th>Best Hyperparams</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adaboost</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': None, 'learning_rate': 0.1, 'n_estimators': 100, 'random_state': None}</td>\n",
       "      <td>1.533360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>decision_tree</td>\n",
       "      <td>0.977904</td>\n",
       "      <td>{'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 2, 'min_samples_split': 8, 'min_weight_fraction_leaf': 0.0, 'random_state': None, 'splitter': 'best'}</td>\n",
       "      <td>2.070426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.955043</td>\n",
       "      <td>{'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 2, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 2, 'min_samples_split': 10, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}</td>\n",
       "      <td>4.799752</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Model  Best Score  \\\n",
       "0       adaboost    1.000000   \n",
       "1  decision_tree    0.977904   \n",
       "2  random_forest    0.955043   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                         Best Hyperparams  \\\n",
       "0                                                                                                                                                                                                                                                                                       {'algorithm': 'SAMME.R', 'base_estimator': None, 'learning_rate': 0.1, 'n_estimators': 100, 'random_state': None}   \n",
       "1                                                                                                                     {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 2, 'min_samples_split': 8, 'min_weight_fraction_leaf': 0.0, 'random_state': None, 'splitter': 'best'}   \n",
       "2  {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 2, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 2, 'min_samples_split': 10, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}   \n",
       "\n",
       "       Time  \n",
       "0  1.533360  \n",
       "1  2.070426  \n",
       "2  4.799752  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = datafactory.finetune(X, y, method='native', models=models, cv=5, mtype='C')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9731807",
   "metadata": {},
   "source": [
    "Here we defined a custom search space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "932d113c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list with params for every model to try out (search strategy of hyperparameters should be in ['grid', 'random'])\n",
    "params = {'strategy': 'random', 'decision_tree': {\"criterion\": ['gini', 'entropy'], \"max_depth\": range(1, 50), \"min_samples_split\": range(1, 20), \"min_samples_leaf\": range(1, 5)}, 'random_forest': {'max_depth': [1, 2, 3, 5, 10, 20, 50], 'min_samples_leaf': [1, 5, 10], 'min_samples_split': [2, 5, 10], 'n_estimators': [50, 100, 200]}, 'adaboost': {'n_estimators': [50, 100, 200], 'learning_rate':[0.001,0.01,.1]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "851c456c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Best Score</th>\n",
       "      <th>Best Hyperparams</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 10, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 10, 'min_samples_split': 10, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 50, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}</td>\n",
       "      <td>4.266476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>decision_tree</td>\n",
       "      <td>0.912026</td>\n",
       "      <td>{'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 14, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 13, 'min_weight_fraction_leaf': 0.0, 'random_state': None, 'splitter': 'best'}</td>\n",
       "      <td>0.250030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adaboost</td>\n",
       "      <td>0.911111</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': None, 'learning_rate': 0.01, 'n_estimators': 200, 'random_state': None}</td>\n",
       "      <td>1.625872</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Model  Best Score  \\\n",
       "0  random_forest    1.000000   \n",
       "1  decision_tree    0.912026   \n",
       "2       adaboost    0.911111   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                          Best Hyperparams  \\\n",
       "0  {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 10, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 10, 'min_samples_split': 10, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 50, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}   \n",
       "1                                                                                                                 {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 14, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 13, 'min_weight_fraction_leaf': 0.0, 'random_state': None, 'splitter': 'best'}   \n",
       "2                                                                                                                                                                                                                                                                                       {'algorithm': 'SAMME.R', 'base_estimator': None, 'learning_rate': 0.01, 'n_estimators': 200, 'random_state': None}   \n",
       "\n",
       "       Time  \n",
       "0  4.266476  \n",
       "1  0.250030  \n",
       "2  1.625872  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = datafactory.finetune(X, y, method='native', models=models, cv=5, mtype='C', params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef5fbef",
   "metadata": {},
   "source": [
    "### Auto-sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326ab775",
   "metadata": {},
   "source": [
    "Auto-sklearn requires a linux OS (otherwise it can be run on colab). It is an automated machine learning toolkit using sklearn models. It automatically trains different ML models with different hyperparameters. At the end it selects the best model. In the DataFactory you can use it like that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de3192f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, score = datafactory.finetune(X, y, method='auto_sklearn', mtype='C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f795c544",
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datafactory",
   "language": "python",
   "name": "datafactory"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
