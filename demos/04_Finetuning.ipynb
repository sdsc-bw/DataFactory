{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b7de143",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/sdsc-bw/DataFactory/blob/develop/demos/04_Finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fe1f22",
   "metadata": {},
   "source": [
    "# Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7d63d5",
   "metadata": {},
   "source": [
    "To find a suitable model for your ML problem is very important. Not every model has the same performane on every task. Some models can be to simple (underfitting) and some models can be to complex for for a problem (overfitting). Also a model has different hyperparameters which also have an impact on the performance. Therefor exist libraries that can be used to find a appropriate model and its hyperparameters. Popular ones are [auto-sklearn](https://papers.neurips.cc/paper/2015/hash/11d0e6287202fced83f79975ec59a3a6-Abstract.html) and [hyperopt](https://github.com/hyperopt/hyperopt).\n",
    "\n",
    "Not every model fits for every problem. In this notebook we can see the F1 scores of several models on different datasets. \n",
    "\n",
    "The F1 score is the harmonic mean of the precision and the recall: \n",
    "$$F1 = 2 * \\frac{precision * recall}{precision + recall}$$\n",
    "The higher the F1 score, the better the prediction. Precision and recall are defined as:\n",
    "\n",
    "$$Precision = \\frac{TP}{TP + FP}, Recall = \\frac{TP}{TP + FN}$$\n",
    "TP: True Positive, FN: False Negative, FP: False Positive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfaae384",
   "metadata": {},
   "source": [
    "# How To use in the Datafactory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8364c25",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e2df4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8205e07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in sys.modules:\n",
    "    !git clone https://github.com/sdsc-bw/DataFactory.git\n",
    "    !ls\n",
    "    \n",
    "    !sudo apt-get install build-essential swig\n",
    "    !curl https://raw.githubusercontent.com/automl/auto-sklearn/master/requirements.txt | xargs -n 1 -L 1 pip install\n",
    "    !pip install auto-sklearn\n",
    "    \n",
    "    !pip install scipy\n",
    "    \n",
    "    !pip install tsai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46a0a167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9289706",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-15 15:33:54,312 - numba.cuda.cudadrv.driver - init\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.utils import shuffle\n",
    "from hyperopt import hp\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    root = 'DataFactory/'\n",
    "else:\n",
    "    root = '../'\n",
    "sys.path.append(root)\n",
    "\n",
    "from datafactory.preprocessing.cleaning import clean_data\n",
    "from datafactory.preprocessing.loading import split_data\n",
    "from datafactory.finetuning.finetuning_hyperopt import finetune_hyperopt\n",
    "from datafactory.finetuning.finetuning_auto_sklearn import finetune_auto_sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9259c4c0",
   "metadata": {},
   "source": [
    "## Load dataset: Diabetes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0eb83d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-15 15:33:57,000 - datafactory.util.constants - Start to clean the given dataframe...\n",
      "2022-01-15 15:33:57,002 - datafactory.util.constants - Number of INF- and NAN-values are: (0, 0)\n",
      "2022-01-15 15:33:57,003 - datafactory.util.constants - Set type to float32 at first && deal with INF\n",
      "2022-01-15 15:33:57,004 - datafactory.util.constants - Remove columns with half of NAN-values\n",
      "2022-01-15 15:33:57,006 - datafactory.util.constants - Remove constant columns\n",
      "2022-01-15 15:33:57,010 - datafactory.util.constants - ...End with Data cleaning, number of INF- and NAN-values are now: (0, 0)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_wine()\n",
    "df = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
    "df['class'] = pd.Series(dataset.target)\n",
    "df = clean_data(df)\n",
    "X, y, _ = split_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527d1d73",
   "metadata": {},
   "source": [
    "## How To Use DataFactory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5253ff",
   "metadata": {},
   "source": [
    "### Hyperopt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100cca6b",
   "metadata": {},
   "source": [
    "We provided a function to use hyperopt. Some of the models require finetuning with hyperopt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "515994ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list with models to try out\n",
    "models = ['decision_tree', 'random_forest', 'ada_boost', 'inception_time', 'res_net']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da3da630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Score</th>\n",
       "      <th>Hyperparams</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ResNet</td>\n",
       "      <td>0.990741</td>\n",
       "      <td>{'epochs': 25, 'lr_max': 0.001}</td>\n",
       "      <td>11.937192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.983051</td>\n",
       "      <td>{'max_depth': 5, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 100}</td>\n",
       "      <td>0.430953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.977495</td>\n",
       "      <td>{'max_depth': 50, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 50}</td>\n",
       "      <td>0.185529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.977495</td>\n",
       "      <td>{'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 5, 'n_estimators': 200}</td>\n",
       "      <td>0.690807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.977401</td>\n",
       "      <td>{'max_depth': 2, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 200}</td>\n",
       "      <td>0.659948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.966196</td>\n",
       "      <td>{'max_depth': 20, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 50}</td>\n",
       "      <td>0.266305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.966196</td>\n",
       "      <td>{'max_depth': 3, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 100}</td>\n",
       "      <td>0.428028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ResNet</td>\n",
       "      <td>0.935185</td>\n",
       "      <td>{'epochs': 25, 'lr_max': 0.001}</td>\n",
       "      <td>11.431798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ResNet</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>{'epochs': 25, 'lr_max': 0.001}</td>\n",
       "      <td>11.544339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ResNet</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>{'epochs': 25, 'lr_max': 0.001}</td>\n",
       "      <td>11.414284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.915631</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 6.0, 'min_samples_leaf': 2, 'min_samples_split': 3}</td>\n",
       "      <td>0.047873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.910075</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 6.0, 'min_samples_leaf': 2, 'min_samples_split': 2}</td>\n",
       "      <td>0.021914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.898776</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 10.0, 'min_samples_leaf': 2, 'min_samples_split': 2}</td>\n",
       "      <td>0.025959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ResNet</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>{'epochs': 25, 'lr_max': 0.001}</td>\n",
       "      <td>11.530027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ResNet</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>{'epochs': 25, 'lr_max': 0.001}</td>\n",
       "      <td>11.211223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.888136</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 7.0, 'min_samples_leaf': 2, 'min_samples_split': 5}</td>\n",
       "      <td>0.053858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.888136</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 9.0, 'min_samples_leaf': 2, 'min_samples_split': 3}</td>\n",
       "      <td>0.064827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.888136</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 3.0, 'min_samples_leaf': 4, 'min_samples_split': 2}</td>\n",
       "      <td>0.027953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.888136</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 5.0, 'min_samples_leaf': 2, 'min_samples_split': 2}</td>\n",
       "      <td>0.045877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ResNet</td>\n",
       "      <td>0.879630</td>\n",
       "      <td>{'epochs': 25, 'lr_max': 0.001}</td>\n",
       "      <td>11.295912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.877024</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 4.0, 'min_samples_leaf': 2, 'min_samples_split': 5}</td>\n",
       "      <td>0.022940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.876836</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 8.0, 'min_samples_leaf': 4, 'min_samples_split': 3}</td>\n",
       "      <td>0.051371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.871375</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 7.0, 'min_samples_leaf': 1, 'min_samples_split': 5}</td>\n",
       "      <td>0.026929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.865537</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 7.0, 'min_samples_leaf': 4, 'min_samples_split': 2}</td>\n",
       "      <td>0.044880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.865537</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 5.0, 'min_samples_leaf': 4, 'min_samples_split': 2}</td>\n",
       "      <td>0.026930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.832203</td>\n",
       "      <td>{'learning_rate': 0.001, 'n_estimators': 100}</td>\n",
       "      <td>0.379513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.832203</td>\n",
       "      <td>{'learning_rate': 0.001, 'n_estimators': 100}</td>\n",
       "      <td>0.448879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.831638</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 2.0, 'min_samples_leaf': 4, 'min_samples_split': 3}</td>\n",
       "      <td>0.030918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>InceptionTime</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>{'epochs': 25, 'lr_max': 0.001, 'nb_filters': 64, 'nf': 32}</td>\n",
       "      <td>18.218433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>InceptionTime</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>{'epochs': 25, 'lr_max': 0.001, 'nb_filters': 32, 'nf': 64}</td>\n",
       "      <td>28.972042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>InceptionTime</td>\n",
       "      <td>0.759259</td>\n",
       "      <td>{'epochs': 25, 'lr_max': 0.001, 'nb_filters': 128, 'nf': 32}</td>\n",
       "      <td>17.648311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>InceptionTime</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>{'epochs': 25, 'lr_max': 0.001, 'nb_filters': 32, 'nf': 32}</td>\n",
       "      <td>17.530056</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Model     Score  \\\n",
       "0          ResNet  0.990741   \n",
       "1   Random Forest  0.983051   \n",
       "2   Random Forest  0.977495   \n",
       "3   Random Forest  0.977495   \n",
       "4   Random Forest  0.977401   \n",
       "5   Random Forest  0.966196   \n",
       "6   Random Forest  0.966196   \n",
       "7          ResNet  0.935185   \n",
       "8          ResNet  0.916667   \n",
       "9          ResNet  0.916667   \n",
       "10  Decision Tree  0.915631   \n",
       "11  Decision Tree  0.910075   \n",
       "12  Decision Tree  0.898776   \n",
       "13         ResNet  0.888889   \n",
       "14         ResNet  0.888889   \n",
       "15  Decision Tree  0.888136   \n",
       "16  Decision Tree  0.888136   \n",
       "17  Decision Tree  0.888136   \n",
       "18  Decision Tree  0.888136   \n",
       "19         ResNet  0.879630   \n",
       "20  Decision Tree  0.877024   \n",
       "21  Decision Tree  0.876836   \n",
       "22  Decision Tree  0.871375   \n",
       "23  Decision Tree  0.865537   \n",
       "24  Decision Tree  0.865537   \n",
       "25       AdaBoost  0.832203   \n",
       "26       AdaBoost  0.832203   \n",
       "27  Decision Tree  0.831638   \n",
       "28  InceptionTime  0.814815   \n",
       "29  InceptionTime  0.814815   \n",
       "30  InceptionTime  0.759259   \n",
       "31  InceptionTime  0.740741   \n",
       "\n",
       "                                                                                  Hyperparams  \\\n",
       "0                                                             {'epochs': 25, 'lr_max': 0.001}   \n",
       "1        {'max_depth': 5, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 100}   \n",
       "2       {'max_depth': 50, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 50}   \n",
       "3       {'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 5, 'n_estimators': 200}   \n",
       "4       {'max_depth': 2, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 200}   \n",
       "5       {'max_depth': 20, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 50}   \n",
       "6        {'max_depth': 3, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 100}   \n",
       "7                                                             {'epochs': 25, 'lr_max': 0.001}   \n",
       "8                                                             {'epochs': 25, 'lr_max': 0.001}   \n",
       "9                                                             {'epochs': 25, 'lr_max': 0.001}   \n",
       "10     {'criterion': 'gini', 'max_depth': 6.0, 'min_samples_leaf': 2, 'min_samples_split': 3}   \n",
       "11     {'criterion': 'gini', 'max_depth': 6.0, 'min_samples_leaf': 2, 'min_samples_split': 2}   \n",
       "12    {'criterion': 'gini', 'max_depth': 10.0, 'min_samples_leaf': 2, 'min_samples_split': 2}   \n",
       "13                                                            {'epochs': 25, 'lr_max': 0.001}   \n",
       "14                                                            {'epochs': 25, 'lr_max': 0.001}   \n",
       "15  {'criterion': 'entropy', 'max_depth': 7.0, 'min_samples_leaf': 2, 'min_samples_split': 5}   \n",
       "16  {'criterion': 'entropy', 'max_depth': 9.0, 'min_samples_leaf': 2, 'min_samples_split': 3}   \n",
       "17  {'criterion': 'entropy', 'max_depth': 3.0, 'min_samples_leaf': 4, 'min_samples_split': 2}   \n",
       "18  {'criterion': 'entropy', 'max_depth': 5.0, 'min_samples_leaf': 2, 'min_samples_split': 2}   \n",
       "19                                                            {'epochs': 25, 'lr_max': 0.001}   \n",
       "20  {'criterion': 'entropy', 'max_depth': 4.0, 'min_samples_leaf': 2, 'min_samples_split': 5}   \n",
       "21     {'criterion': 'gini', 'max_depth': 8.0, 'min_samples_leaf': 4, 'min_samples_split': 3}   \n",
       "22  {'criterion': 'entropy', 'max_depth': 7.0, 'min_samples_leaf': 1, 'min_samples_split': 5}   \n",
       "23     {'criterion': 'gini', 'max_depth': 7.0, 'min_samples_leaf': 4, 'min_samples_split': 2}   \n",
       "24     {'criterion': 'gini', 'max_depth': 5.0, 'min_samples_leaf': 4, 'min_samples_split': 2}   \n",
       "25                                              {'learning_rate': 0.001, 'n_estimators': 100}   \n",
       "26                                              {'learning_rate': 0.001, 'n_estimators': 100}   \n",
       "27     {'criterion': 'gini', 'max_depth': 2.0, 'min_samples_leaf': 4, 'min_samples_split': 3}   \n",
       "28                                {'epochs': 25, 'lr_max': 0.001, 'nb_filters': 64, 'nf': 32}   \n",
       "29                                {'epochs': 25, 'lr_max': 0.001, 'nb_filters': 32, 'nf': 64}   \n",
       "30                               {'epochs': 25, 'lr_max': 0.001, 'nb_filters': 128, 'nf': 32}   \n",
       "31                                {'epochs': 25, 'lr_max': 0.001, 'nb_filters': 32, 'nf': 32}   \n",
       "\n",
       "         Time  \n",
       "0   11.937192  \n",
       "1    0.430953  \n",
       "2    0.185529  \n",
       "3    0.690807  \n",
       "4    0.659948  \n",
       "5    0.266305  \n",
       "6    0.428028  \n",
       "7   11.431798  \n",
       "8   11.544339  \n",
       "9   11.414284  \n",
       "10   0.047873  \n",
       "11   0.021914  \n",
       "12   0.025959  \n",
       "13  11.530027  \n",
       "14  11.211223  \n",
       "15   0.053858  \n",
       "16   0.064827  \n",
       "17   0.027953  \n",
       "18   0.045877  \n",
       "19  11.295912  \n",
       "20   0.022940  \n",
       "21   0.051371  \n",
       "22   0.026929  \n",
       "23   0.044880  \n",
       "24   0.026930  \n",
       "25   0.379513  \n",
       "26   0.448879  \n",
       "27   0.030918  \n",
       "28  18.218433  \n",
       "29  28.972042  \n",
       "30  17.648311  \n",
       "31  17.530056  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 32/32 [02:47<00:00,  5.24s/trial, best loss: -0.9907407363255819]\n"
     ]
    }
   ],
   "source": [
    "# loss in this case refers to -f1\n",
    "model = finetune_hyperopt(X, y, strategy='random', models=models, cv=5, max_evals=32, mtype='C')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2265eb63",
   "metadata": {},
   "source": [
    "We can define the models that we want to test. Then we have to define a *params* variable that defines the strategy how to examine the search space. There we also can define the parameters of the search space. If parameters for models are not given, it uses our standard search space.\n",
    "\n",
    "If we want to define custom parameters, they should be defined with the functions of hyperopt. Look at the [sklearn](https://scikit-learn.org/stable/) and [tsai](https://github.com/timeseriesAI/tsai) website to find the hyperparamters of the models. Attention: The identifier of the hyperparameters need to be unique for ever parameter (also between models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b60853f",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['decision_tree', 'random_forest', 'ada_boost', 'inception_time', 'res_net']\n",
    "# attention the label has to be unique for every parameter (also between models)\n",
    "decision_tree_params = {'max_depth': hp.quniform('max_depth_dt', 1, 10, 1), \n",
    "                        'criterion': hp.choice('criterion_dt', ['gini', 'entropy']), \n",
    "                        'min_samples_leaf': hp.choice('min_samples_leaf_dt', [1, 2, 4])}\n",
    "random_forest_params = {'max_depth': hp.choice('max_depth_rf', [1, 2, 3, 5, 10, 20, 50]), \n",
    "                        'n_estimators': hp.choice('n_estimators_rf', [50, 100, 200])}\n",
    "ada_boost_params = {'n_estimators': hp.choice('n_estimators_ab', [50, 100, 200]), \n",
    "                    'learning_rate': hp.choice('learning_rate_ab', [0.001,0.01,.1,1.0])}\n",
    "inception_time_params = {'epochs': hp.choice('epochs_it', [50, 100, 150]), \n",
    "                         'lr_max': 1e-3, \n",
    "                         'opt_func':  hp.choice('optimizer_it', ['adam', 'sgd']), \n",
    "                         'loss_func': hp.choice('loss_it', ['cross_entropy', 'smooth_cross_entropy']), \n",
    "                         'batch_tfms': hp.choice('batch_tfms_it', [['standardize', 'clip', 'mag_scale'], []]), \n",
    "                         'batch_size': [64, 128], \n",
    "                         'splits': None, \n",
    "                         'metrics': ['accuracy'],\n",
    "                         'nf': hp.choice('nf_it', [32, 64]), \n",
    "                         'nb_filters': hp.choice('nb_filters_it', [32, 64, 96, 128])}\n",
    "res_net_params = {'epochs': hp.choice('epochs__res_net', [50, 100, 150]), \n",
    "                  'lr_max': 1e-3, \n",
    "                  'opt_func':  hp.choice('optimizer__res_net', ['adam', 'sgd']), \n",
    "                  'loss_func': hp.choice('loss__res_net', ['cross_entropy', 'smooth_cross_entropy']), \n",
    "                  'batch_tfms': hp.choice('batch_tfms_res_net', [['standardize', 'clip', 'mag_scale'], []]), \n",
    "                  'batch_size': [64, 128], \n",
    "                  'splits': None, \n",
    "                  'metrics': ['accuracy']}\n",
    "# put every hyperparameter definition in an own dictionary\n",
    "params = {'decision_tree': decision_tree_params, \n",
    "          'random_forest': random_forest_params, \n",
    "          'ada_boost': ada_boost_params, \n",
    "          'inception_time': inception_time_params, \n",
    "          'res_net': res_net_params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3676483",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Score</th>\n",
       "      <th>Hyperparams</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ResNet</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{'batch_size': (64, 128), 'batch_tfms': (), 'epochs': 150, 'loss_func': 'smooth_cross_entropy', 'lr_max': 0.001, 'opt_func': 'sgd', 'splits': None}</td>\n",
       "      <td>68.652344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>InceptionTime</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{'batch_size': (64, 128), 'batch_tfms': ('standardize', 'clip', 'mag_scale'), 'epochs': 150, 'loss_func': 'cross_entropy', 'lr_max': 0.001, 'nb_filters': 64, 'nf': 32, 'opt_func': 'adam', 'splits': None}</td>\n",
       "      <td>104.737286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ResNet</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{'batch_size': (64, 128), 'batch_tfms': (), 'epochs': 150, 'loss_func': 'cross_entropy', 'lr_max': 0.001, 'opt_func': 'adam', 'splits': None}</td>\n",
       "      <td>79.153059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>InceptionTime</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{'batch_size': (64, 128), 'batch_tfms': (), 'epochs': 100, 'loss_func': 'cross_entropy', 'lr_max': 0.001, 'nb_filters': 64, 'nf': 32, 'opt_func': 'sgd', 'splits': None}</td>\n",
       "      <td>70.050056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ResNet</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{'batch_size': (64, 128), 'batch_tfms': (), 'epochs': 150, 'loss_func': 'cross_entropy', 'lr_max': 0.001, 'opt_func': 'sgd', 'splits': None}</td>\n",
       "      <td>77.701328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>InceptionTime</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{'batch_size': (64, 128), 'batch_tfms': (), 'epochs': 150, 'loss_func': 'cross_entropy', 'lr_max': 0.001, 'nb_filters': 64, 'nf': 64, 'opt_func': 'adam', 'splits': None}</td>\n",
       "      <td>183.858962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ResNet</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{'batch_size': (64, 128), 'batch_tfms': (), 'epochs': 50, 'loss_func': 'smooth_cross_entropy', 'lr_max': 0.001, 'opt_func': 'sgd', 'splits': None}</td>\n",
       "      <td>22.298556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ResNet</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{'batch_size': (64, 128), 'batch_tfms': (), 'epochs': 50, 'loss_func': 'smooth_cross_entropy', 'lr_max': 0.001, 'opt_func': 'sgd', 'splits': None}</td>\n",
       "      <td>29.456243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ResNet</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{'batch_size': (64, 128), 'batch_tfms': ('standardize', 'clip', 'mag_scale'), 'epochs': 50, 'loss_func': 'smooth_cross_entropy', 'lr_max': 0.001, 'opt_func': 'adam', 'splits': None}</td>\n",
       "      <td>23.208064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ResNet</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{'batch_size': (64, 128), 'batch_tfms': (), 'epochs': 100, 'loss_func': 'cross_entropy', 'lr_max': 0.001, 'opt_func': 'adam', 'splits': None}</td>\n",
       "      <td>48.454832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.988701</td>\n",
       "      <td>{'max_depth': 10, 'n_estimators': 50}</td>\n",
       "      <td>0.204473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.988701</td>\n",
       "      <td>{'max_depth': 10, 'n_estimators': 100}</td>\n",
       "      <td>0.423388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.983051</td>\n",
       "      <td>{'max_depth': 50, 'n_estimators': 50}</td>\n",
       "      <td>0.183077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ResNet</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>{'batch_size': (64, 128), 'batch_tfms': ('standardize', 'clip', 'mag_scale'), 'epochs': 50, 'loss_func': 'smooth_cross_entropy', 'lr_max': 0.001, 'opt_func': 'adam', 'splits': None}</td>\n",
       "      <td>24.905188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.977495</td>\n",
       "      <td>{'max_depth': 3, 'n_estimators': 50}</td>\n",
       "      <td>0.267629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ResNet</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>{'batch_size': (64, 128), 'batch_tfms': ('standardize', 'clip', 'mag_scale'), 'epochs': 50, 'loss_func': 'cross_entropy', 'lr_max': 0.001, 'opt_func': 'sgd', 'splits': None}</td>\n",
       "      <td>25.550815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.960546</td>\n",
       "      <td>{'max_depth': 2, 'n_estimators': 50}</td>\n",
       "      <td>0.191982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.926648</td>\n",
       "      <td>{'learning_rate': 0.01, 'n_estimators': 100}</td>\n",
       "      <td>0.360035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>InceptionTime</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>{'batch_size': (64, 128), 'batch_tfms': (), 'epochs': 50, 'loss_func': 'cross_entropy', 'lr_max': 0.001, 'nb_filters': 32, 'nf': 32, 'opt_func': 'adam', 'splits': None}</td>\n",
       "      <td>33.701453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.916008</td>\n",
       "      <td>{'learning_rate': 0.1, 'n_estimators': 50}</td>\n",
       "      <td>0.276707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.915537</td>\n",
       "      <td>{'learning_rate': 0.01, 'n_estimators': 200}</td>\n",
       "      <td>0.721091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.910075</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 5.0, 'min_samples_leaf': 2}</td>\n",
       "      <td>0.029889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>InceptionTime</td>\n",
       "      <td>0.907407</td>\n",
       "      <td>{'batch_size': (64, 128), 'batch_tfms': ('standardize', 'clip', 'mag_scale'), 'epochs': 50, 'loss_func': 'smooth_cross_entropy', 'lr_max': 0.001, 'nb_filters': 96, 'nf': 64, 'opt_func': 'adam', 'splits': None}</td>\n",
       "      <td>60.143996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.904520</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 6.0, 'min_samples_leaf': 1}</td>\n",
       "      <td>0.069815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.899058</td>\n",
       "      <td>{'learning_rate': 0.1, 'n_estimators': 100}</td>\n",
       "      <td>0.366799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.888136</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 7.0, 'min_samples_leaf': 2}</td>\n",
       "      <td>0.037717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.887853</td>\n",
       "      <td>{'learning_rate': 0.1, 'n_estimators': 200}</td>\n",
       "      <td>0.784904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.865537</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 8.0, 'min_samples_leaf': 4}</td>\n",
       "      <td>0.066823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.865066</td>\n",
       "      <td>{'learning_rate': 1.0, 'n_estimators': 50}</td>\n",
       "      <td>0.203484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>ResNet</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>{'batch_size': (64, 128), 'batch_tfms': ('standardize', 'clip', 'mag_scale'), 'epochs': 100, 'loss_func': 'smooth_cross_entropy', 'lr_max': 0.001, 'opt_func': 'sgd', 'splits': None}</td>\n",
       "      <td>44.836826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>ResNet</td>\n",
       "      <td>0.796296</td>\n",
       "      <td>{'batch_size': (64, 128), 'batch_tfms': ('standardize', 'clip', 'mag_scale'), 'epochs': 150, 'loss_func': 'smooth_cross_entropy', 'lr_max': 0.001, 'opt_func': 'sgd', 'splits': None}</td>\n",
       "      <td>78.344418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>ResNet</td>\n",
       "      <td>0.620370</td>\n",
       "      <td>{'batch_size': (64, 128), 'batch_tfms': ('standardize', 'clip', 'mag_scale'), 'epochs': 100, 'loss_func': 'cross_entropy', 'lr_max': 0.001, 'opt_func': 'adam', 'splits': None}</td>\n",
       "      <td>47.092803</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Model     Score  \\\n",
       "0          ResNet  1.000000   \n",
       "1   InceptionTime  1.000000   \n",
       "2          ResNet  1.000000   \n",
       "3   InceptionTime  1.000000   \n",
       "4          ResNet  1.000000   \n",
       "5   InceptionTime  1.000000   \n",
       "6          ResNet  1.000000   \n",
       "7          ResNet  1.000000   \n",
       "8          ResNet  1.000000   \n",
       "9          ResNet  1.000000   \n",
       "10  Random Forest  0.988701   \n",
       "11  Random Forest  0.988701   \n",
       "12  Random Forest  0.983051   \n",
       "13         ResNet  0.981481   \n",
       "14  Random Forest  0.977495   \n",
       "15         ResNet  0.972222   \n",
       "16  Random Forest  0.960546   \n",
       "17       AdaBoost  0.926648   \n",
       "18  InceptionTime  0.916667   \n",
       "19       AdaBoost  0.916008   \n",
       "20       AdaBoost  0.915537   \n",
       "21  Decision Tree  0.910075   \n",
       "22  InceptionTime  0.907407   \n",
       "23  Decision Tree  0.904520   \n",
       "24       AdaBoost  0.899058   \n",
       "25  Decision Tree  0.888136   \n",
       "26       AdaBoost  0.887853   \n",
       "27  Decision Tree  0.865537   \n",
       "28       AdaBoost  0.865066   \n",
       "29         ResNet  0.851852   \n",
       "30         ResNet  0.796296   \n",
       "31         ResNet  0.620370   \n",
       "\n",
       "                                                                                                                                                                                                          Hyperparams  \\\n",
       "0                                                                 {'batch_size': (64, 128), 'batch_tfms': (), 'epochs': 150, 'loss_func': 'smooth_cross_entropy', 'lr_max': 0.001, 'opt_func': 'sgd', 'splits': None}   \n",
       "1         {'batch_size': (64, 128), 'batch_tfms': ('standardize', 'clip', 'mag_scale'), 'epochs': 150, 'loss_func': 'cross_entropy', 'lr_max': 0.001, 'nb_filters': 64, 'nf': 32, 'opt_func': 'adam', 'splits': None}   \n",
       "2                                                                       {'batch_size': (64, 128), 'batch_tfms': (), 'epochs': 150, 'loss_func': 'cross_entropy', 'lr_max': 0.001, 'opt_func': 'adam', 'splits': None}   \n",
       "3                                            {'batch_size': (64, 128), 'batch_tfms': (), 'epochs': 100, 'loss_func': 'cross_entropy', 'lr_max': 0.001, 'nb_filters': 64, 'nf': 32, 'opt_func': 'sgd', 'splits': None}   \n",
       "4                                                                        {'batch_size': (64, 128), 'batch_tfms': (), 'epochs': 150, 'loss_func': 'cross_entropy', 'lr_max': 0.001, 'opt_func': 'sgd', 'splits': None}   \n",
       "5                                           {'batch_size': (64, 128), 'batch_tfms': (), 'epochs': 150, 'loss_func': 'cross_entropy', 'lr_max': 0.001, 'nb_filters': 64, 'nf': 64, 'opt_func': 'adam', 'splits': None}   \n",
       "6                                                                  {'batch_size': (64, 128), 'batch_tfms': (), 'epochs': 50, 'loss_func': 'smooth_cross_entropy', 'lr_max': 0.001, 'opt_func': 'sgd', 'splits': None}   \n",
       "7                                                                  {'batch_size': (64, 128), 'batch_tfms': (), 'epochs': 50, 'loss_func': 'smooth_cross_entropy', 'lr_max': 0.001, 'opt_func': 'sgd', 'splits': None}   \n",
       "8                               {'batch_size': (64, 128), 'batch_tfms': ('standardize', 'clip', 'mag_scale'), 'epochs': 50, 'loss_func': 'smooth_cross_entropy', 'lr_max': 0.001, 'opt_func': 'adam', 'splits': None}   \n",
       "9                                                                       {'batch_size': (64, 128), 'batch_tfms': (), 'epochs': 100, 'loss_func': 'cross_entropy', 'lr_max': 0.001, 'opt_func': 'adam', 'splits': None}   \n",
       "10                                                                                                                                                                              {'max_depth': 10, 'n_estimators': 50}   \n",
       "11                                                                                                                                                                             {'max_depth': 10, 'n_estimators': 100}   \n",
       "12                                                                                                                                                                              {'max_depth': 50, 'n_estimators': 50}   \n",
       "13                              {'batch_size': (64, 128), 'batch_tfms': ('standardize', 'clip', 'mag_scale'), 'epochs': 50, 'loss_func': 'smooth_cross_entropy', 'lr_max': 0.001, 'opt_func': 'adam', 'splits': None}   \n",
       "14                                                                                                                                                                               {'max_depth': 3, 'n_estimators': 50}   \n",
       "15                                      {'batch_size': (64, 128), 'batch_tfms': ('standardize', 'clip', 'mag_scale'), 'epochs': 50, 'loss_func': 'cross_entropy', 'lr_max': 0.001, 'opt_func': 'sgd', 'splits': None}   \n",
       "16                                                                                                                                                                               {'max_depth': 2, 'n_estimators': 50}   \n",
       "17                                                                                                                                                                       {'learning_rate': 0.01, 'n_estimators': 100}   \n",
       "18                                           {'batch_size': (64, 128), 'batch_tfms': (), 'epochs': 50, 'loss_func': 'cross_entropy', 'lr_max': 0.001, 'nb_filters': 32, 'nf': 32, 'opt_func': 'adam', 'splits': None}   \n",
       "19                                                                                                                                                                         {'learning_rate': 0.1, 'n_estimators': 50}   \n",
       "20                                                                                                                                                                       {'learning_rate': 0.01, 'n_estimators': 200}   \n",
       "21                                                                                                                                                     {'criterion': 'gini', 'max_depth': 5.0, 'min_samples_leaf': 2}   \n",
       "22  {'batch_size': (64, 128), 'batch_tfms': ('standardize', 'clip', 'mag_scale'), 'epochs': 50, 'loss_func': 'smooth_cross_entropy', 'lr_max': 0.001, 'nb_filters': 96, 'nf': 64, 'opt_func': 'adam', 'splits': None}   \n",
       "23                                                                                                                                                     {'criterion': 'gini', 'max_depth': 6.0, 'min_samples_leaf': 1}   \n",
       "24                                                                                                                                                                        {'learning_rate': 0.1, 'n_estimators': 100}   \n",
       "25                                                                                                                                                  {'criterion': 'entropy', 'max_depth': 7.0, 'min_samples_leaf': 2}   \n",
       "26                                                                                                                                                                        {'learning_rate': 0.1, 'n_estimators': 200}   \n",
       "27                                                                                                                                                     {'criterion': 'gini', 'max_depth': 8.0, 'min_samples_leaf': 4}   \n",
       "28                                                                                                                                                                         {'learning_rate': 1.0, 'n_estimators': 50}   \n",
       "29                              {'batch_size': (64, 128), 'batch_tfms': ('standardize', 'clip', 'mag_scale'), 'epochs': 100, 'loss_func': 'smooth_cross_entropy', 'lr_max': 0.001, 'opt_func': 'sgd', 'splits': None}   \n",
       "30                              {'batch_size': (64, 128), 'batch_tfms': ('standardize', 'clip', 'mag_scale'), 'epochs': 150, 'loss_func': 'smooth_cross_entropy', 'lr_max': 0.001, 'opt_func': 'sgd', 'splits': None}   \n",
       "31                                    {'batch_size': (64, 128), 'batch_tfms': ('standardize', 'clip', 'mag_scale'), 'epochs': 100, 'loss_func': 'cross_entropy', 'lr_max': 0.001, 'opt_func': 'adam', 'splits': None}   \n",
       "\n",
       "          Time  \n",
       "0    68.652344  \n",
       "1   104.737286  \n",
       "2    79.153059  \n",
       "3    70.050056  \n",
       "4    77.701328  \n",
       "5   183.858962  \n",
       "6    22.298556  \n",
       "7    29.456243  \n",
       "8    23.208064  \n",
       "9    48.454832  \n",
       "10    0.204473  \n",
       "11    0.423388  \n",
       "12    0.183077  \n",
       "13   24.905188  \n",
       "14    0.267629  \n",
       "15   25.550815  \n",
       "16    0.191982  \n",
       "17    0.360035  \n",
       "18   33.701453  \n",
       "19    0.276707  \n",
       "20    0.721091  \n",
       "21    0.029889  \n",
       "22   60.143996  \n",
       "23    0.069815  \n",
       "24    0.366799  \n",
       "25    0.037717  \n",
       "26    0.784904  \n",
       "27    0.066823  \n",
       "28    0.203484  \n",
       "29   44.836826  \n",
       "30   78.344418  \n",
       "31   47.092803  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████| 32/32 [17:07<00:00, 32.11s/trial, best loss: -1.0]\n"
     ]
    }
   ],
   "source": [
    "# search strategy should be in ['parzen', 'random']\n",
    "model = finetune_hyperopt(X, y, strategy='random', models=models, cv=5, mtype='C', params=params.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef5fbef",
   "metadata": {},
   "source": [
    "### Auto-sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326ab775",
   "metadata": {},
   "source": [
    "Auto-sklearn requires a linux OS (otherwise it can be run on colab). It is an automated machine learning toolkit using sklearn models. It automatically trains different ML models with different hyperparameters. At the end it selects the best model. In the DataFactory you can use it like that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de3192f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = finetune_auto_sklearn(X, y, mtype='C')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datafactory",
   "language": "python",
   "name": "datafactory"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
