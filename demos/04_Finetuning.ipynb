{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b7de143",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/sdsc-bw/DataFactory/blob/develop/demos/04_Finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fe1f22",
   "metadata": {},
   "source": [
    "# Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7d63d5",
   "metadata": {},
   "source": [
    "To find a suitable model for your ML problem is very important. Not every model has the same performane on every task. Some models can be to simple (underfitting) and some models can be to complex for for a problem (overfitting). Also a model has different hyperparameters which also have an impact on the performance. Therefor exist libraries that can be used to find a appropriate model and its hyperparameters. In this github we use [hyperopt](https://github.com/hyperopt/hyperopt).\n",
    "\n",
    "Not every model fits for every problem. In this notebook we can see the F1 scores of several models on different datasets. \n",
    "\n",
    "The F1 score is the harmonic mean of the precision and the recall: \n",
    "$$F1 = 2 * \\frac{precision * recall}{precision + recall}$$\n",
    "The higher the F1 score, the better the prediction. Precision and recall are defined as:\n",
    "\n",
    "$$Precision = \\frac{TP}{TP + FP}, Recall = \\frac{TP}{TP + FN}$$\n",
    "TP: True Positive, FN: False Negative, FP: False Positive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfaae384",
   "metadata": {},
   "source": [
    "# How To use in the Datafactory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8364c25",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8205e07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if running in colab\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    !git clone https://github.com/sdsc-bw/DataFactory.git # clone repository for colab\n",
    "    !ls\n",
    "    \n",
    "    !pip3 install scipy==1.5 # install scipy to use hyperopt, RESTART RUNTIME AFTER THAT\n",
    "    \n",
    "    !pip3 install mlflow # install mlflow to use hyperopt\n",
    "    \n",
    "    # install auto-sklearn\n",
    "    !sudo apt-get install build-essential swig\n",
    "    !curl https://raw.githubusercontent.com/automl/auto-sklearn/master/requirements.txt | xargs -n 1 -L 1 pip install\n",
    "    !pip install auto-sklearn\n",
    "    \n",
    "    !pip install tsai # install tsai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46a0a167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings # igorne irrelevant warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9289706",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # library used for visualization\n",
    "import pandas as pd # library for creating tables\n",
    "from hyperopt import hp # libary for finetuning and defining search spaces\n",
    "from sklearn.datasets import load_wine # wine dataset\n",
    "\n",
    "## add path to import datafactory \n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    root = 'DataFactory/'\n",
    "else:\n",
    "    root = '../'\n",
    "sys.path.append(root)\n",
    "\n",
    "from datafactory.preprocessing.cleaning import clean_data # method to clean data\n",
    "from datafactory.preprocessing.loading import split_data # method to split into training and test data\n",
    "from datafactory.finetuning.finetuning_hyperopt import finetune_hyperopt # method to finetune with hyperopt\n",
    "from datafactory.finetuning.finetuning_auto_sklearn import finetune_auto_sklearn # method to finetune with auto-sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9259c4c0",
   "metadata": {},
   "source": [
    "## Load dataset: Wine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0eb83d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-16 16:10:27,807 - Start to clean the given dataframe...\n",
      "2022-01-16 16:10:27,811 - Number of INF- and NAN-values are: (0, 0)\n",
      "2022-01-16 16:10:27,812 - Set type to float32 at first && deal with INF\n",
      "2022-01-16 16:10:27,814 - Remove columns with half of NAN-values\n",
      "2022-01-16 16:10:27,816 - Remove constant columns\n",
      "2022-01-16 16:10:27,821 - ...End with Data cleaning, number of INF- and NAN-values are now: (0, 0)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_wine()\n",
    "df = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
    "df['class'] = pd.Series(dataset.target)\n",
    "df = clean_data(df)\n",
    "X, y, _ = split_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527d1d73",
   "metadata": {},
   "source": [
    "## How To Use DataFactory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5253ff",
   "metadata": {},
   "source": [
    "### Hyperopt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100cca6b",
   "metadata": {},
   "source": [
    "We provided a function to use hyperopt for finetuning. You can just create a list with models which you want to try out. We provide a standard search space for every model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "515994ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list with models to try out\n",
    "models = ['decision_tree', 'random_forest', 'ada_boost', 'inception_time', 'res_net']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da3da630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Score</th>\n",
       "      <th>Hyperparams</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.988889</td>\n",
       "      <td>{'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 100}</td>\n",
       "      <td>0.565484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>{'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 200}</td>\n",
       "      <td>1.141511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.983175</td>\n",
       "      <td>{'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200}</td>\n",
       "      <td>1.069688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.983175</td>\n",
       "      <td>{'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}</td>\n",
       "      <td>1.121022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.983175</td>\n",
       "      <td>{'max_depth': 5, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 200}</td>\n",
       "      <td>1.068254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.977619</td>\n",
       "      <td>{'max_depth': 2, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 100}</td>\n",
       "      <td>0.567314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.977619</td>\n",
       "      <td>{'max_depth': 2, 'min_samples_leaf': 4, 'min_samples_split': 5, 'n_estimators': 100}</td>\n",
       "      <td>0.634305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.966508</td>\n",
       "      <td>{'max_depth': 2, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 50}</td>\n",
       "      <td>0.289225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.966349</td>\n",
       "      <td>{'max_depth': 5, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 200}</td>\n",
       "      <td>1.083142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.955397</td>\n",
       "      <td>{'max_depth': 1, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50}</td>\n",
       "      <td>0.374096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.927143</td>\n",
       "      <td>{'learning_rate': 0.01, 'n_estimators': 100}</td>\n",
       "      <td>0.627322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.915873</td>\n",
       "      <td>{'learning_rate': 0.01, 'n_estimators': 50}</td>\n",
       "      <td>0.313147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.910476</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 7.0, 'min_samples_leaf': 2, 'min_samples_split': 3}</td>\n",
       "      <td>0.042451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.899048</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 7.0, 'min_samples_leaf': 2, 'min_samples_split': 5}</td>\n",
       "      <td>0.077301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.894127</td>\n",
       "      <td>{'learning_rate': 0.1, 'n_estimators': 50}</td>\n",
       "      <td>0.337935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.894127</td>\n",
       "      <td>{'learning_rate': 0.1, 'n_estimators': 50}</td>\n",
       "      <td>0.334470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.894127</td>\n",
       "      <td>{'learning_rate': 0.1, 'n_estimators': 50}</td>\n",
       "      <td>0.319139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.888095</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 2.0, 'min_samples_leaf': 2, 'min_samples_split': 3}</td>\n",
       "      <td>0.075309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.842698</td>\n",
       "      <td>{'learning_rate': 0.001, 'n_estimators': 200}</td>\n",
       "      <td>1.330779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.831270</td>\n",
       "      <td>{'learning_rate': 0.001, 'n_estimators': 100}</td>\n",
       "      <td>0.679203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>{'learning_rate': 0.001, 'n_estimators': 50}</td>\n",
       "      <td>0.320246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ResNet</td>\n",
       "      <td>0.772222</td>\n",
       "      <td>{'epochs': 25, 'lr_max': 0.001}</td>\n",
       "      <td>17.969914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ResNet</td>\n",
       "      <td>0.772222</td>\n",
       "      <td>{'epochs': 25, 'lr_max': 0.001}</td>\n",
       "      <td>19.281970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ResNet</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>{'epochs': 25, 'lr_max': 0.001}</td>\n",
       "      <td>19.913464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ResNet</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>{'epochs': 25, 'lr_max': 0.001}</td>\n",
       "      <td>20.078831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>InceptionTime</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>{'epochs': 25, 'lr_max': 0.001, 'nb_filters': 32, 'nf': 64}</td>\n",
       "      <td>49.485085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>InceptionTime</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>{'epochs': 25, 'lr_max': 0.001, 'nb_filters': 64, 'nf': 32}</td>\n",
       "      <td>26.483229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>InceptionTime</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>{'epochs': 25, 'lr_max': 0.001, 'nb_filters': 96, 'nf': 32}</td>\n",
       "      <td>28.563096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>InceptionTime</td>\n",
       "      <td>0.627778</td>\n",
       "      <td>{'epochs': 25, 'lr_max': 0.001, 'nb_filters': 32, 'nf': 32}</td>\n",
       "      <td>27.641340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>InceptionTime</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>{'epochs': 25, 'lr_max': 0.001, 'nb_filters': 32, 'nf': 64}</td>\n",
       "      <td>45.340741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>InceptionTime</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>{'epochs': 25, 'lr_max': 0.001, 'nb_filters': 64, 'nf': 64}</td>\n",
       "      <td>50.169698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>InceptionTime</td>\n",
       "      <td>0.605556</td>\n",
       "      <td>{'epochs': 25, 'lr_max': 0.001, 'nb_filters': 128, 'nf': 64}</td>\n",
       "      <td>50.708003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Model     Score  \\\n",
       "0   Random Forest  0.988889   \n",
       "1   Random Forest  0.983333   \n",
       "2   Random Forest  0.983175   \n",
       "3   Random Forest  0.983175   \n",
       "4   Random Forest  0.983175   \n",
       "5   Random Forest  0.977619   \n",
       "6   Random Forest  0.977619   \n",
       "7   Random Forest  0.966508   \n",
       "8   Random Forest  0.966349   \n",
       "9   Random Forest  0.955397   \n",
       "10       AdaBoost  0.927143   \n",
       "11       AdaBoost  0.915873   \n",
       "12  Decision Tree  0.910476   \n",
       "13  Decision Tree  0.899048   \n",
       "14       AdaBoost  0.894127   \n",
       "15       AdaBoost  0.894127   \n",
       "16       AdaBoost  0.894127   \n",
       "17  Decision Tree  0.888095   \n",
       "18       AdaBoost  0.842698   \n",
       "19       AdaBoost  0.831270   \n",
       "20       AdaBoost  0.820000   \n",
       "21         ResNet  0.772222   \n",
       "22         ResNet  0.772222   \n",
       "23         ResNet  0.766667   \n",
       "24         ResNet  0.750000   \n",
       "25  InceptionTime  0.638889   \n",
       "26  InceptionTime  0.633333   \n",
       "27  InceptionTime  0.633333   \n",
       "28  InceptionTime  0.627778   \n",
       "29  InceptionTime  0.616667   \n",
       "30  InceptionTime  0.616667   \n",
       "31  InceptionTime  0.605556   \n",
       "\n",
       "                                                                                  Hyperparams  \\\n",
       "0       {'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 100}   \n",
       "1       {'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 200}   \n",
       "2      {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200}   \n",
       "3        {'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}   \n",
       "4        {'max_depth': 5, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 200}   \n",
       "5        {'max_depth': 2, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 100}   \n",
       "6        {'max_depth': 2, 'min_samples_leaf': 4, 'min_samples_split': 5, 'n_estimators': 100}   \n",
       "7        {'max_depth': 2, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 50}   \n",
       "8       {'max_depth': 5, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 200}   \n",
       "9         {'max_depth': 1, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50}   \n",
       "10                                               {'learning_rate': 0.01, 'n_estimators': 100}   \n",
       "11                                                {'learning_rate': 0.01, 'n_estimators': 50}   \n",
       "12  {'criterion': 'entropy', 'max_depth': 7.0, 'min_samples_leaf': 2, 'min_samples_split': 3}   \n",
       "13     {'criterion': 'gini', 'max_depth': 7.0, 'min_samples_leaf': 2, 'min_samples_split': 5}   \n",
       "14                                                 {'learning_rate': 0.1, 'n_estimators': 50}   \n",
       "15                                                 {'learning_rate': 0.1, 'n_estimators': 50}   \n",
       "16                                                 {'learning_rate': 0.1, 'n_estimators': 50}   \n",
       "17  {'criterion': 'entropy', 'max_depth': 2.0, 'min_samples_leaf': 2, 'min_samples_split': 3}   \n",
       "18                                              {'learning_rate': 0.001, 'n_estimators': 200}   \n",
       "19                                              {'learning_rate': 0.001, 'n_estimators': 100}   \n",
       "20                                               {'learning_rate': 0.001, 'n_estimators': 50}   \n",
       "21                                                            {'epochs': 25, 'lr_max': 0.001}   \n",
       "22                                                            {'epochs': 25, 'lr_max': 0.001}   \n",
       "23                                                            {'epochs': 25, 'lr_max': 0.001}   \n",
       "24                                                            {'epochs': 25, 'lr_max': 0.001}   \n",
       "25                                {'epochs': 25, 'lr_max': 0.001, 'nb_filters': 32, 'nf': 64}   \n",
       "26                                {'epochs': 25, 'lr_max': 0.001, 'nb_filters': 64, 'nf': 32}   \n",
       "27                                {'epochs': 25, 'lr_max': 0.001, 'nb_filters': 96, 'nf': 32}   \n",
       "28                                {'epochs': 25, 'lr_max': 0.001, 'nb_filters': 32, 'nf': 32}   \n",
       "29                                {'epochs': 25, 'lr_max': 0.001, 'nb_filters': 32, 'nf': 64}   \n",
       "30                                {'epochs': 25, 'lr_max': 0.001, 'nb_filters': 64, 'nf': 64}   \n",
       "31                               {'epochs': 25, 'lr_max': 0.001, 'nb_filters': 128, 'nf': 64}   \n",
       "\n",
       "         Time  \n",
       "0    0.565484  \n",
       "1    1.141511  \n",
       "2    1.069688  \n",
       "3    1.121022  \n",
       "4    1.068254  \n",
       "5    0.567314  \n",
       "6    0.634305  \n",
       "7    0.289225  \n",
       "8    1.083142  \n",
       "9    0.374096  \n",
       "10   0.627322  \n",
       "11   0.313147  \n",
       "12   0.042451  \n",
       "13   0.077301  \n",
       "14   0.337935  \n",
       "15   0.334470  \n",
       "16   0.319139  \n",
       "17   0.075309  \n",
       "18   1.330779  \n",
       "19   0.679203  \n",
       "20   0.320246  \n",
       "21  17.969914  \n",
       "22  19.281970  \n",
       "23  19.913464  \n",
       "24  20.078831  \n",
       "25  49.485085  \n",
       "26  26.483229  \n",
       "27  28.563096  \n",
       "28  27.641340  \n",
       "29  45.340741  \n",
       "30  50.169698  \n",
       "31  50.708003  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 32/32 [06:08<00:00, 11.53s/trial, best loss: -0.9888888888888889]\n"
     ]
    }
   ],
   "source": [
    "# loss in this case refers to -f1\n",
    "# search strategy should be in ['parzen', 'random']\n",
    "model = finetune_hyperopt(X, y, strategy='random', models=models, cv=5, max_evals=32, mtype='C')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2265eb63",
   "metadata": {},
   "source": [
    "Is you want to use your custom search space, then you have to define a *params* variable that defines the parameters to try out. If parameters for models are not given, it uses our standard search space.\n",
    "\n",
    "If you want to define custom parameters, they should be defined with the functions of hyperopt. Look at the [sklearn](https://scikit-learn.org/stable/) and [tsai](https://github.com/timeseriesAI/tsai) website to find the hyperparamters of the models. Attention: The identifier of the hyperparameters need to be unique for ever parameter (also between models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b60853f",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['decision_tree', 'random_forest', 'ada_boost', 'inception_time', 'res_net']\n",
    "# attention the label has to be unique for every parameter (also between models)\n",
    "decision_tree_params = {'max_depth': hp.quniform('max_depth_dt', 1, 10, 1), \n",
    "                        'criterion': hp.choice('criterion_dt', ['gini', 'entropy']), \n",
    "                        'min_samples_leaf': hp.choice('min_samples_leaf_dt', [1, 2, 4])}\n",
    "random_forest_params = {'max_depth': hp.choice('max_depth_rf', [1, 2, 3, 5, 10, 20, 50]), \n",
    "                        'n_estimators': hp.choice('n_estimators_rf', [50, 100, 200])}\n",
    "ada_boost_params = {'n_estimators': hp.choice('n_estimators_ab', [50, 100, 200]), \n",
    "                    'learning_rate': hp.choice('learning_rate_ab', [0.001,0.01,.1,1.0])}\n",
    "inception_time_params = {'epochs': hp.choice('epochs_it', [50, 100, 150]), \n",
    "                         'lr_max': 1e-3, \n",
    "                         'opt_func':  hp.choice('optimizer_it', ['adam', 'sgd']), \n",
    "                         'loss_func': hp.choice('loss_it', ['cross_entropy', 'smooth_cross_entropy']), \n",
    "                         'batch_tfms': hp.choice('batch_tfms_it', [['standardize', 'clip', 'mag_scale'], []]), \n",
    "                         'batch_size': [64, 128], \n",
    "                         'splits': None, \n",
    "                         'metrics': ['accuracy'],\n",
    "                         'nf': hp.choice('nf_it', [32, 64]), \n",
    "                         'nb_filters': hp.choice('nb_filters_it', [32, 64, 96, 128])}\n",
    "res_net_params = {'epochs': hp.choice('epochs__res_net', [50, 100, 150]), \n",
    "                  'lr_max': 1e-3, \n",
    "                  'opt_func':  hp.choice('optimizer__res_net', ['adam', 'sgd']), \n",
    "                  'loss_func': hp.choice('loss__res_net', ['cross_entropy', 'smooth_cross_entropy']), \n",
    "                  'batch_tfms': hp.choice('batch_tfms_res_net', [['standardize', 'clip', 'mag_scale'], []]), \n",
    "                  'batch_size': [64, 128], \n",
    "                  'splits': None, \n",
    "                  'metrics': ['accuracy']}\n",
    "# put every hyperparameter definition in an own dictionary\n",
    "params = {'decision_tree': decision_tree_params, \n",
    "          'random_forest': random_forest_params, \n",
    "          'ada_boost': ada_boost_params, \n",
    "          'inception_time': inception_time_params, \n",
    "          'res_net': res_net_params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3676483",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Score</th>\n",
       "      <th>Hyperparams</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ResNet</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{'batch_size': (64, 128), 'batch_tfms': (), 'epochs': 150, 'loss_func': 'smooth_cross_entropy', 'lr_max': 0.001, 'opt_func': 'sgd', 'splits': None}</td>\n",
       "      <td>68.652344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>InceptionTime</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{'batch_size': (64, 128), 'batch_tfms': ('standardize', 'clip', 'mag_scale'), 'epochs': 150, 'loss_func': 'cross_entropy', 'lr_max': 0.001, 'nb_filters': 64, 'nf': 32, 'opt_func': 'adam', 'splits': None}</td>\n",
       "      <td>104.737286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ResNet</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{'batch_size': (64, 128), 'batch_tfms': (), 'epochs': 150, 'loss_func': 'cross_entropy', 'lr_max': 0.001, 'opt_func': 'adam', 'splits': None}</td>\n",
       "      <td>79.153059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>InceptionTime</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{'batch_size': (64, 128), 'batch_tfms': (), 'epochs': 100, 'loss_func': 'cross_entropy', 'lr_max': 0.001, 'nb_filters': 64, 'nf': 32, 'opt_func': 'sgd', 'splits': None}</td>\n",
       "      <td>70.050056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ResNet</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{'batch_size': (64, 128), 'batch_tfms': (), 'epochs': 150, 'loss_func': 'cross_entropy', 'lr_max': 0.001, 'opt_func': 'sgd', 'splits': None}</td>\n",
       "      <td>77.701328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>InceptionTime</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{'batch_size': (64, 128), 'batch_tfms': (), 'epochs': 150, 'loss_func': 'cross_entropy', 'lr_max': 0.001, 'nb_filters': 64, 'nf': 64, 'opt_func': 'adam', 'splits': None}</td>\n",
       "      <td>183.858962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ResNet</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{'batch_size': (64, 128), 'batch_tfms': (), 'epochs': 50, 'loss_func': 'smooth_cross_entropy', 'lr_max': 0.001, 'opt_func': 'sgd', 'splits': None}</td>\n",
       "      <td>22.298556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ResNet</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{'batch_size': (64, 128), 'batch_tfms': (), 'epochs': 50, 'loss_func': 'smooth_cross_entropy', 'lr_max': 0.001, 'opt_func': 'sgd', 'splits': None}</td>\n",
       "      <td>29.456243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ResNet</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{'batch_size': (64, 128), 'batch_tfms': ('standardize', 'clip', 'mag_scale'), 'epochs': 50, 'loss_func': 'smooth_cross_entropy', 'lr_max': 0.001, 'opt_func': 'adam', 'splits': None}</td>\n",
       "      <td>23.208064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ResNet</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{'batch_size': (64, 128), 'batch_tfms': (), 'epochs': 100, 'loss_func': 'cross_entropy', 'lr_max': 0.001, 'opt_func': 'adam', 'splits': None}</td>\n",
       "      <td>48.454832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.988701</td>\n",
       "      <td>{'max_depth': 10, 'n_estimators': 50}</td>\n",
       "      <td>0.204473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.988701</td>\n",
       "      <td>{'max_depth': 10, 'n_estimators': 100}</td>\n",
       "      <td>0.423388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.983051</td>\n",
       "      <td>{'max_depth': 50, 'n_estimators': 50}</td>\n",
       "      <td>0.183077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ResNet</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>{'batch_size': (64, 128), 'batch_tfms': ('standardize', 'clip', 'mag_scale'), 'epochs': 50, 'loss_func': 'smooth_cross_entropy', 'lr_max': 0.001, 'opt_func': 'adam', 'splits': None}</td>\n",
       "      <td>24.905188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.977495</td>\n",
       "      <td>{'max_depth': 3, 'n_estimators': 50}</td>\n",
       "      <td>0.267629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ResNet</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>{'batch_size': (64, 128), 'batch_tfms': ('standardize', 'clip', 'mag_scale'), 'epochs': 50, 'loss_func': 'cross_entropy', 'lr_max': 0.001, 'opt_func': 'sgd', 'splits': None}</td>\n",
       "      <td>25.550815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.960546</td>\n",
       "      <td>{'max_depth': 2, 'n_estimators': 50}</td>\n",
       "      <td>0.191982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.926648</td>\n",
       "      <td>{'learning_rate': 0.01, 'n_estimators': 100}</td>\n",
       "      <td>0.360035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>InceptionTime</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>{'batch_size': (64, 128), 'batch_tfms': (), 'epochs': 50, 'loss_func': 'cross_entropy', 'lr_max': 0.001, 'nb_filters': 32, 'nf': 32, 'opt_func': 'adam', 'splits': None}</td>\n",
       "      <td>33.701453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.916008</td>\n",
       "      <td>{'learning_rate': 0.1, 'n_estimators': 50}</td>\n",
       "      <td>0.276707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.915537</td>\n",
       "      <td>{'learning_rate': 0.01, 'n_estimators': 200}</td>\n",
       "      <td>0.721091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.910075</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 5.0, 'min_samples_leaf': 2}</td>\n",
       "      <td>0.029889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>InceptionTime</td>\n",
       "      <td>0.907407</td>\n",
       "      <td>{'batch_size': (64, 128), 'batch_tfms': ('standardize', 'clip', 'mag_scale'), 'epochs': 50, 'loss_func': 'smooth_cross_entropy', 'lr_max': 0.001, 'nb_filters': 96, 'nf': 64, 'opt_func': 'adam', 'splits': None}</td>\n",
       "      <td>60.143996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.904520</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 6.0, 'min_samples_leaf': 1}</td>\n",
       "      <td>0.069815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.899058</td>\n",
       "      <td>{'learning_rate': 0.1, 'n_estimators': 100}</td>\n",
       "      <td>0.366799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.888136</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 7.0, 'min_samples_leaf': 2}</td>\n",
       "      <td>0.037717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.887853</td>\n",
       "      <td>{'learning_rate': 0.1, 'n_estimators': 200}</td>\n",
       "      <td>0.784904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.865537</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 8.0, 'min_samples_leaf': 4}</td>\n",
       "      <td>0.066823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.865066</td>\n",
       "      <td>{'learning_rate': 1.0, 'n_estimators': 50}</td>\n",
       "      <td>0.203484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>ResNet</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>{'batch_size': (64, 128), 'batch_tfms': ('standardize', 'clip', 'mag_scale'), 'epochs': 100, 'loss_func': 'smooth_cross_entropy', 'lr_max': 0.001, 'opt_func': 'sgd', 'splits': None}</td>\n",
       "      <td>44.836826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>ResNet</td>\n",
       "      <td>0.796296</td>\n",
       "      <td>{'batch_size': (64, 128), 'batch_tfms': ('standardize', 'clip', 'mag_scale'), 'epochs': 150, 'loss_func': 'smooth_cross_entropy', 'lr_max': 0.001, 'opt_func': 'sgd', 'splits': None}</td>\n",
       "      <td>78.344418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>ResNet</td>\n",
       "      <td>0.620370</td>\n",
       "      <td>{'batch_size': (64, 128), 'batch_tfms': ('standardize', 'clip', 'mag_scale'), 'epochs': 100, 'loss_func': 'cross_entropy', 'lr_max': 0.001, 'opt_func': 'adam', 'splits': None}</td>\n",
       "      <td>47.092803</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Model     Score  \\\n",
       "0          ResNet  1.000000   \n",
       "1   InceptionTime  1.000000   \n",
       "2          ResNet  1.000000   \n",
       "3   InceptionTime  1.000000   \n",
       "4          ResNet  1.000000   \n",
       "5   InceptionTime  1.000000   \n",
       "6          ResNet  1.000000   \n",
       "7          ResNet  1.000000   \n",
       "8          ResNet  1.000000   \n",
       "9          ResNet  1.000000   \n",
       "10  Random Forest  0.988701   \n",
       "11  Random Forest  0.988701   \n",
       "12  Random Forest  0.983051   \n",
       "13         ResNet  0.981481   \n",
       "14  Random Forest  0.977495   \n",
       "15         ResNet  0.972222   \n",
       "16  Random Forest  0.960546   \n",
       "17       AdaBoost  0.926648   \n",
       "18  InceptionTime  0.916667   \n",
       "19       AdaBoost  0.916008   \n",
       "20       AdaBoost  0.915537   \n",
       "21  Decision Tree  0.910075   \n",
       "22  InceptionTime  0.907407   \n",
       "23  Decision Tree  0.904520   \n",
       "24       AdaBoost  0.899058   \n",
       "25  Decision Tree  0.888136   \n",
       "26       AdaBoost  0.887853   \n",
       "27  Decision Tree  0.865537   \n",
       "28       AdaBoost  0.865066   \n",
       "29         ResNet  0.851852   \n",
       "30         ResNet  0.796296   \n",
       "31         ResNet  0.620370   \n",
       "\n",
       "                                                                                                                                                                                                          Hyperparams  \\\n",
       "0                                                                 {'batch_size': (64, 128), 'batch_tfms': (), 'epochs': 150, 'loss_func': 'smooth_cross_entropy', 'lr_max': 0.001, 'opt_func': 'sgd', 'splits': None}   \n",
       "1         {'batch_size': (64, 128), 'batch_tfms': ('standardize', 'clip', 'mag_scale'), 'epochs': 150, 'loss_func': 'cross_entropy', 'lr_max': 0.001, 'nb_filters': 64, 'nf': 32, 'opt_func': 'adam', 'splits': None}   \n",
       "2                                                                       {'batch_size': (64, 128), 'batch_tfms': (), 'epochs': 150, 'loss_func': 'cross_entropy', 'lr_max': 0.001, 'opt_func': 'adam', 'splits': None}   \n",
       "3                                            {'batch_size': (64, 128), 'batch_tfms': (), 'epochs': 100, 'loss_func': 'cross_entropy', 'lr_max': 0.001, 'nb_filters': 64, 'nf': 32, 'opt_func': 'sgd', 'splits': None}   \n",
       "4                                                                        {'batch_size': (64, 128), 'batch_tfms': (), 'epochs': 150, 'loss_func': 'cross_entropy', 'lr_max': 0.001, 'opt_func': 'sgd', 'splits': None}   \n",
       "5                                           {'batch_size': (64, 128), 'batch_tfms': (), 'epochs': 150, 'loss_func': 'cross_entropy', 'lr_max': 0.001, 'nb_filters': 64, 'nf': 64, 'opt_func': 'adam', 'splits': None}   \n",
       "6                                                                  {'batch_size': (64, 128), 'batch_tfms': (), 'epochs': 50, 'loss_func': 'smooth_cross_entropy', 'lr_max': 0.001, 'opt_func': 'sgd', 'splits': None}   \n",
       "7                                                                  {'batch_size': (64, 128), 'batch_tfms': (), 'epochs': 50, 'loss_func': 'smooth_cross_entropy', 'lr_max': 0.001, 'opt_func': 'sgd', 'splits': None}   \n",
       "8                               {'batch_size': (64, 128), 'batch_tfms': ('standardize', 'clip', 'mag_scale'), 'epochs': 50, 'loss_func': 'smooth_cross_entropy', 'lr_max': 0.001, 'opt_func': 'adam', 'splits': None}   \n",
       "9                                                                       {'batch_size': (64, 128), 'batch_tfms': (), 'epochs': 100, 'loss_func': 'cross_entropy', 'lr_max': 0.001, 'opt_func': 'adam', 'splits': None}   \n",
       "10                                                                                                                                                                              {'max_depth': 10, 'n_estimators': 50}   \n",
       "11                                                                                                                                                                             {'max_depth': 10, 'n_estimators': 100}   \n",
       "12                                                                                                                                                                              {'max_depth': 50, 'n_estimators': 50}   \n",
       "13                              {'batch_size': (64, 128), 'batch_tfms': ('standardize', 'clip', 'mag_scale'), 'epochs': 50, 'loss_func': 'smooth_cross_entropy', 'lr_max': 0.001, 'opt_func': 'adam', 'splits': None}   \n",
       "14                                                                                                                                                                               {'max_depth': 3, 'n_estimators': 50}   \n",
       "15                                      {'batch_size': (64, 128), 'batch_tfms': ('standardize', 'clip', 'mag_scale'), 'epochs': 50, 'loss_func': 'cross_entropy', 'lr_max': 0.001, 'opt_func': 'sgd', 'splits': None}   \n",
       "16                                                                                                                                                                               {'max_depth': 2, 'n_estimators': 50}   \n",
       "17                                                                                                                                                                       {'learning_rate': 0.01, 'n_estimators': 100}   \n",
       "18                                           {'batch_size': (64, 128), 'batch_tfms': (), 'epochs': 50, 'loss_func': 'cross_entropy', 'lr_max': 0.001, 'nb_filters': 32, 'nf': 32, 'opt_func': 'adam', 'splits': None}   \n",
       "19                                                                                                                                                                         {'learning_rate': 0.1, 'n_estimators': 50}   \n",
       "20                                                                                                                                                                       {'learning_rate': 0.01, 'n_estimators': 200}   \n",
       "21                                                                                                                                                     {'criterion': 'gini', 'max_depth': 5.0, 'min_samples_leaf': 2}   \n",
       "22  {'batch_size': (64, 128), 'batch_tfms': ('standardize', 'clip', 'mag_scale'), 'epochs': 50, 'loss_func': 'smooth_cross_entropy', 'lr_max': 0.001, 'nb_filters': 96, 'nf': 64, 'opt_func': 'adam', 'splits': None}   \n",
       "23                                                                                                                                                     {'criterion': 'gini', 'max_depth': 6.0, 'min_samples_leaf': 1}   \n",
       "24                                                                                                                                                                        {'learning_rate': 0.1, 'n_estimators': 100}   \n",
       "25                                                                                                                                                  {'criterion': 'entropy', 'max_depth': 7.0, 'min_samples_leaf': 2}   \n",
       "26                                                                                                                                                                        {'learning_rate': 0.1, 'n_estimators': 200}   \n",
       "27                                                                                                                                                     {'criterion': 'gini', 'max_depth': 8.0, 'min_samples_leaf': 4}   \n",
       "28                                                                                                                                                                         {'learning_rate': 1.0, 'n_estimators': 50}   \n",
       "29                              {'batch_size': (64, 128), 'batch_tfms': ('standardize', 'clip', 'mag_scale'), 'epochs': 100, 'loss_func': 'smooth_cross_entropy', 'lr_max': 0.001, 'opt_func': 'sgd', 'splits': None}   \n",
       "30                              {'batch_size': (64, 128), 'batch_tfms': ('standardize', 'clip', 'mag_scale'), 'epochs': 150, 'loss_func': 'smooth_cross_entropy', 'lr_max': 0.001, 'opt_func': 'sgd', 'splits': None}   \n",
       "31                                    {'batch_size': (64, 128), 'batch_tfms': ('standardize', 'clip', 'mag_scale'), 'epochs': 100, 'loss_func': 'cross_entropy', 'lr_max': 0.001, 'opt_func': 'adam', 'splits': None}   \n",
       "\n",
       "          Time  \n",
       "0    68.652344  \n",
       "1   104.737286  \n",
       "2    79.153059  \n",
       "3    70.050056  \n",
       "4    77.701328  \n",
       "5   183.858962  \n",
       "6    22.298556  \n",
       "7    29.456243  \n",
       "8    23.208064  \n",
       "9    48.454832  \n",
       "10    0.204473  \n",
       "11    0.423388  \n",
       "12    0.183077  \n",
       "13   24.905188  \n",
       "14    0.267629  \n",
       "15   25.550815  \n",
       "16    0.191982  \n",
       "17    0.360035  \n",
       "18   33.701453  \n",
       "19    0.276707  \n",
       "20    0.721091  \n",
       "21    0.029889  \n",
       "22   60.143996  \n",
       "23    0.069815  \n",
       "24    0.366799  \n",
       "25    0.037717  \n",
       "26    0.784904  \n",
       "27    0.066823  \n",
       "28    0.203484  \n",
       "29   44.836826  \n",
       "30   78.344418  \n",
       "31   47.092803  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████| 32/32 [17:07<00:00, 32.11s/trial, best loss: -1.0]\n"
     ]
    }
   ],
   "source": [
    "# search strategy should be in ['parzen', 'random']\n",
    "model = finetune_hyperopt(X, y, strategy='random', models=models, cv=5, mtype='C', params=params.copy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datafactory",
   "language": "python",
   "name": "datafactory"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
